#!/usr/bin/env python3
# scripts/build_akcent.py
# -*- coding: utf-8 -*-

"""
–ü–æ–ª–Ω—ã–π —Å–±–æ—Ä—â–∏–∫ —Ñ–∏–¥–∞ Akcent (YML/XML) c –ø–∞—Ç—á–µ–º –ø–æ ¬´–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å¬ª:
‚Äî –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–æ ¬´–∑–∞—Ö–≤–∞—Ç—ã–≤–∞–Ω–∏–µ¬ª –ø–æ—Å–ª–µ–¥—É—é—â–∏—Ö —Å–µ–∫—Ü–∏–π (–ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã/–ê—É–¥–∏–æ/–ü–∏—Ç–∞–Ω–∏–µ/‚Ä¶) –≤ –∑–Ω–∞—á–µ–Ω–∏–µ ¬´–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å¬ª.
‚Äî –î–æ–±–∞–≤–ª–µ–Ω —Å–ø–∏—Å–æ–∫ SECTION_BREAKERS –∏ —Ñ—É–Ω–∫—Ü–∏—è _is_section_header(), –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ extract_kv_specs_and_clean_native().
‚Äî –û—Å—Ç–∞–ª—å–Ω–æ–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª: —Ñ–∏–ª—å—Ç—Ä –Ω–∞–∑–≤–∞–Ω–∏–π, —Ñ–∏–ª—å—Ç—Ä <param> –¥–ª—è Satu, SEO-–±–ª–æ–∫ (–º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∞—Ç—å ENV), —Ä–µ–ø—Ä–∞–π—Å–∏–Ω–≥, –ø–ª–µ–π—Å—Ö–æ–ª–¥–µ—Ä—ã, keywords.

–ó–∞–ø—É—Å–∫:
  python scripts/build_akcent.py

ENV (–≤–∞–∂–Ω—ã–µ):
  SUPPLIER_URL="..." | OUT_FILE="docs/akcent.yml" | OUTPUT_ENCODING="windows-1251" –∏–ª–∏ "utf-8"
  AKCENT_KEYWORDS_MODE=include|exclude|off
  PARAM_FILTER_ENABLE=1|0
  SEO_STICKY=1|0  (–º–æ–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å SEO-–≤—Å—Ç–∞–≤–∫–∏)
"""

from __future__ import annotations

import os, sys, re, time, json, random, hashlib, urllib.parse
from typing import Optional, List, Tuple, Dict
from copy import deepcopy
from datetime import datetime, timezone, timedelta
from html import unescape as _unescape
from xml.etree import ElementTree as ET

try:
    import requests
except Exception:
    print("ERROR: 'requests' is required (pip install requests)", file=sys.stderr)
    raise

try:
    from zoneinfo import ZoneInfo
except Exception:
    ZoneInfo = None

SCRIPT_VERSION = "akcent-2025-10-27.v4.1.0"

# ===================== ENV / CONST =====================
SUPPLIER_NAME = os.getenv("SUPPLIER_NAME", "Akcent").strip()
SUPPLIER_URL  = os.getenv("SUPPLIER_URL", "https://ak-cent.kz/export/Exchange/article_nw2/Ware02224.xml").strip()
OUT_FILE_YML  = os.getenv("OUT_FILE", "docs/akcent.yml").strip()
ENC           = os.getenv("OUTPUT_ENCODING", "windows-1251").strip()
TIMEOUT_S     = int(os.getenv("TIMEOUT_S", "30"))
RETRIES       = int(os.getenv("RETRIES", "4"))
RETRY_BACKOFF = float(os.getenv("RETRY_BACKOFF_S", "2"))
MIN_BYTES     = int(os.getenv("MIN_BYTES", "1500"))
DRY_RUN       = os.getenv("DRY_RUN", "0").lower() in {"1","true","yes"}

# ---- –§–∏–ª—å—Ç—Ä –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º (file + env) ----
AKCENT_KEYWORDS_PATH  = os.getenv("AKCENT_KEYWORDS_PATH", "docs/akcent_keywords.txt").strip()
AKCENT_KEYWORDS_MODE  = os.getenv("AKCENT_KEYWORDS_MODE", "include").lower()  # include|exclude|off
AKCENT_KEYWORDS_LIST  = [s.strip() for s in os.getenv("AKCENT_KEYWORDS", "").split(",") if s.strip()]

# ---- Pricing ----
PRICE_CAP_THRESHOLD = int(os.getenv("PRICE_CAP_THRESHOLD", "9999999"))
PRICE_CAP_VALUE     = int(os.getenv("PRICE_CAP_VALUE", "100"))
PriceRule = Tuple[int,int,float,int]
PRICING_RULES: List[PriceRule] = [
    (101,10000,4.0,3000),(10001,25000,4.0,4000),(25001,50000,4.0,5000),
    (50001,75000,4.0,7000),(75001,100000,4.0,10000),(100001,150000,4.0,12000),
    (150001,200000,4.0,15000),(200001,300000,4.0,20000),(300001,400000,4.0,25000),
    (400001,500000,4.0,30000),(500001,750000,4.0,40000),(750001,1000000,4.0,50000),
    (1000001,1500000,4.0,70000),(1500001,2000000,4.0,90000),(2000001,100000000,4.0,100000),
]
INTERNAL_PRICE_TAGS=("purchase_price","purchasePrice","wholesale_price","wholesalePrice","opt_price","optPrice",
                     "b2b_price","b2bPrice","supplier_price","supplierPrice","min_price","minPrice","max_price","maxPrice","oldprice")
PRICE_FIELDS_DIRECT=["purchasePrice","purchase_price","wholesalePrice","wholesale_price","opt_price","b2bPrice","b2b_price"]
PRICE_KEYWORDS_DEALER = re.compile(r"(–¥–∏–ª–µ—Ä|dealer|–æ–ø—Ç|wholesale|b2b|–∑–∞–∫—É–ø|purchase|–æ–ø—Ç–æ–≤)", re.I)
PRICE_KEYWORDS_RRP    = re.compile(r"(rrp|—Ä—Ä—Ü|—Ä–æ–∑–Ω–∏—Ü|retail|msrp)", re.I)

# ---- Placeholders ----
PLACEHOLDER_ENABLE        = os.getenv("PLACEHOLDER_ENABLE", "1").lower() in {"1","true","yes","on"}
PLACEHOLDER_BRAND_BASE    = os.getenv("PLACEHOLDER_BRAND_BASE", "https://img.akcent.kz/brand").rstrip("/")
PLACEHOLDER_CATEGORY_BASE = os.getenv("PLACEHOLDER_CATEGORY_BASE", "https://img.akcent.kz/category").rstrip("/")
PLACEHOLDER_DEFAULT_URL   = os.getenv("PLACEHOLDER_DEFAULT_URL", "https://img.akcent.kz/placeholder.jpg").strip()
PLACEHOLDER_EXT           = os.getenv("PLACEHOLDER_EXT", "jpg").strip().lower()
PLACEHOLDER_HEAD_TIMEOUT  = float(os.getenv("PLACEHOLDER_HEAD_TIMEOUT_S", "5"))

# ---- –ß–∏—Å—Ç–∫–∏/–ø–æ—Ä—è–¥–æ–∫ ----
DROP_CATEGORY_ID_TAG   = True
DROP_STOCK_TAGS        = True
PURGE_TAGS_AFTER       = ("Offer_ID","delivery","local_delivery_cost","manufacturer_warranty","model","url","status","Status")
PURGE_OFFER_ATTRS_AFTER= ("type","article")
DESIRED_ORDER          = ["vendorCode","name","price","picture","vendor","currencyId","description","keywords"]

# ---- SEO/–æ–ø–∏—Å–∞–Ω–∏—è/–∫—ç—à ----
DEFAULT_CACHE_PATH="docs/akcent_cache/seo_cache.json"
SEO_CACHE_PATH=os.getenv("SEO_CACHE_PATH", DEFAULT_CACHE_PATH)
SEO_STICKY=os.getenv("SEO_STICKY","1").lower() in {"1","true","yes","on"}
SEO_REFRESH_MODE=os.getenv("SEO_REFRESH_MODE","monthly_1").lower()  # monthly_1|off

# ---- Keywords ----
SATU_KEYWORDS_MAXLEN=1024
SATU_KEYWORDS_GEO=True
SATU_KEYWORDS_GEO_MAX=20
SATU_KEYWORDS_GEO_LAT=True

# ===================== PARAM FILTER (Satu/SEO) =====================
PARAM_FILTER_ENABLE = os.getenv("PARAM_FILTER_ENABLE", "1").lower() in {"1","true","yes","on"}

def _norm_param_name(s: str) -> str:
    return re.sub(r"\s+"," ", (s or "").strip().lower().replace("—ë","–µ"))

DEFAULT_PARAM_WHITELIST = {
    # –ü—Ä–∏–Ω—Ç–µ—Ä—ã/—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å
    "—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å", "—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å –º–æ–¥–µ–ª—è–º–∏", "–ø—Ä–∏–Ω—Ç–µ—Ä—ã", "–ø–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è", "–º–æ–¥–µ–ª–∏",
    "—Ç–∏–ø –ø–µ—á–∞—Ç–∏", "—Ü–≤–µ—Ç –ø–µ—á–∞—Ç–∏", "—Ü–≤–µ—Ç", "—Ä–µ—Å—É—Ä—Å", "—Ä–µ—Å—É—Ä—Å –±–∞—Ä–∞–±–∞–Ω–∞", "—á–µ—Ä–Ω—ã–π —Ä–µ—Å—É—Ä—Å", "—Ü–≤–µ—Ç–Ω–æ–π —Ä–µ—Å—É—Ä—Å",
    # –ö–∞–±–µ–ª–∏/—Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
    "—Ä–∞–∑—ä–µ–º", "—Ä–∞–∑—ä–µ–º—ã", "—Ä–∞–∑—å–µ–º", "—Ä–∞–∑—å–µ–º—ã", "—Ä–∞–∑—ä—ë–º", "—Ä–∞–∑—ä—ë–º—ã",
    "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å", "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã", "–¥–ª–∏–Ω–∞ –∫–∞–±–µ–ª—è", "–º–∞—Ç–µ—Ä–∏–∞–ª",
    # –ü–∞–º—è—Ç—å/–Ω–∞–∫–æ–ø–∏—Ç–µ–ª–∏
    "–µ–º–∫–æ—Å—Ç—å", "–æ–±—ä–µ–º", "–æ–±—ä—ë–º", "—Ñ–æ—Ä–º-—Ñ–∞–∫—Ç–æ—Ä", "—Ñ–æ—Ä–º—Ñ–∞–∫—Ç–æ—Ä", "—Ç–∏–ø –ø–∞–º—è—Ç–∏",
    "—Å–∫–æ—Ä–æ—Å—Ç—å —á—Ç–µ–Ω–∏—è", "—Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏",
    # –ú–æ–Ω–∏—Ç–æ—Ä—ã/–≤–∏–¥–µ–æ
    "–¥–∏–∞–≥–æ–Ω–∞–ª—å", "—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ", "—Ç–∏–ø –º–∞—Ç—Ä–∏—Ü—ã", "—á–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è", "—è—Ä–∫–æ—Å—Ç—å", "–∫–æ–Ω—Ç—Ä–∞—Å—Ç–Ω–æ—Å—Ç—å",
    "–≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞", "—É–≥–æ–ª –æ–±–∑–æ—Ä–∞", "hdr", "–≤–µ—Ä—Å–∏—è hdmi",
    # –ü–æ—Ä—Ç—ã/–±–µ—Å–ø—Ä–æ–≤–æ–¥–Ω–æ–µ
    "usb", "hdmi", "displayport", "dp", "wi-fi", "wi fi", "bluetooth", "bt", "lan", "ethernet",
    # –≠–Ω–µ—Ä–≥–∏—è/—ç–ª–µ–∫—Ç—Ä–∏–∫–∞
    "–º–æ—â–Ω–æ—Å—Ç—å", "–Ω–∞–ø—Ä—è–∂–µ–Ω–∏–µ", "—á–∞—Å—Ç–æ—Ç–∞", "—Å–∏–ª–∞ —Ç–æ–∫–∞", "—ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω–∏–µ",
    # –û–±—â–∏–µ –≤–∞–∂–Ω—ã–µ
    "—Å—Ç—Ä–∞–Ω–∞", "—Å—Ç—Ä–∞–Ω–∞ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å", "–≥–∞—Ä–∞–Ω—Ç–∏—è", "–∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü–∏—è",
    "–≤–µ—Å", "—Ä–∞–∑–º–µ—Ä—ã", "–≥–∞–±–∞—Ä–∏—Ç—ã",
    # –ò–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã –Ω–∞–∫–æ–ø–∏—Ç–µ–ª–µ–π/–∫–∞—Ä—Ç—ã
    "sata", "pcie", "m.2", "m2", "nvme", "micro sd", "sd", "sdhc", "sdxc",
    # –°–µ—Ç–µ–≤–æ–µ/–ø–∏—Ç–∞–Ω–∏–µ
    "poe", "ip", "ip —Ä–µ–π—Ç–∏–Ω–≥", "ip rating",
    # –ê—É–¥–∏–æ/–≤—Å—Ç—Ä–æ–µ–Ω–Ω–æ–µ
    "–º–∏–∫—Ä–æ—Ñ–æ–Ω", "–¥–∏–Ω–∞–º–∏–∫–∏", "–∫–∞–º–µ—Ä–∞",
}
_PARAM_WL_NORM = {_norm_param_name(x) for x in DEFAULT_PARAM_WHITELIST}
_PARAM_ALLOWED_PATTERNS = [re.compile(p, re.I) for p in [
    r"^—Å–æ–≤–º–µ—Å—Ç–∏–º", r"^–ø—Ä–∏–Ω—Ç–µ—Ä", r"^–ø–æ–¥—Ö–æ–¥–∏—Ç", r"^–º–æ–¥–µ–ª",
    r"^—Ü–≤–µ—Ç( –ø–µ—á–∞—Ç–∏)?$", r"^—Ç–∏–ø( –ø–µ—á–∞—Ç–∏| –º–∞—Ç—Ä–∏—Ü—ã)?$", r"^—Ä–µ—Å—É—Ä—Å",
    r"^—Ä–∞–∑(—ä|–µ)–º", r"^–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å(—ã)?$", r"^–¥–∏–∞–≥–æ–Ω–∞–ª", r"^—Ä–∞–∑—Ä–µ—à–µ–Ω–∏",
    r"^—á–∞—Å—Ç–æ—Ç[–∞—ã] (–æ–±–Ω–æ–≤–ª–µ–Ω|–∫–∞–¥—Ä–æ–≤)", r"^—è—Ä–∫–æ—Å—Ç", r"^–∫–æ–Ω—Ç—Ä–∞—Å—Ç", r"^–≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫", r"^—É–≥–æ–ª –æ–±–∑–æ—Ä–∞$", r"^hdr$",
    r"^(usb|hdmi|displayport|dp|wi-?fi|bluetooth|bt|lan|ethernet)$",
    r"^–¥–ª–∏–Ω–∞ –∫–∞–±–µ–ª", r"^–º–∞—Ç–µ—Ä–∏–∞–ª$", r"^(–µ–º–∫–æ—Å—Ç|–æ–±(—ä|)–µ–º|capacity)", r"^—Ñ–æ—Ä–º[- ]?—Ñ–∞–∫—Ç–æ—Ä$",
    r"^—Å–∫–æ—Ä–æ—Å—Ç[—å–∏] (—á—Ç|–∑–∞–ø)", r"^–º–æ—â–Ω–æ—Å—Ç", r"^–Ω–∞–ø—Ä—è–∂–µ–Ω", r"^—á–∞—Å—Ç–æ—Ç", r"^—Å–∏–ª–∞ —Ç–æ–∫–∞", r"^—ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω",
    r"^(–≤–µ—Å|–≥–∞–±–∞—Ä–∏—Ç(—ã)?|—Ä–∞–∑–º–µ—Ä(—ã)?)$", r"^—Å—Ç—Ä–∞–Ω–∞( –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å)?$", r"^–≥–∞—Ä–∞–Ω—Ç–∏", r"^–∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü",
    r"^(sata|pcie|m\.?2|nvme|micro ?sd|sd(hc|xc)?)$", r"^poe$", r"^ip( —Ä–µ–π—Ç–∏–Ω–≥| rating)?$", r"^(–º–∏–∫—Ä–æ—Ñ–æ–Ω|–¥–∏–Ω–∞–º–∏–∫(–∏)?|–∫–∞–º–µ—Ä–∞)$",
]]

def _attr_ci(el: ET.Element, key: str) -> Optional[str]:
    k = key.lower()
    for a,v in el.attrib.items():
        if a.lower()==k:
            return v
    return None

def _param_allowed(name_raw: Optional[str]) -> bool:
    if not name_raw:
        return False
    n = _norm_param_name(name_raw)
    if n in _PARAM_WL_NORM:
        return True
    return any(p.search(n) for p in _PARAM_ALLOWED_PATTERNS)

def filter_params_for_satu(out_shop: ET.Element) -> Tuple[int,int,int]:
    if not PARAM_FILTER_ENABLE:
        return (0,0,0)
    off_el = out_shop.find("offers")
    if off_el is None:
        return (0,0,0)
    touched = kept = dropped = 0
    for offer in off_el.findall("offer"):
        changed=False
        for node in list(offer):
            if node.tag.lower()!="param":
                continue
            name_val = _attr_ci(node, "name")
            if _param_allowed(name_val):
                kept += 1
                continue
            offer.remove(node)
            dropped += 1
            changed=True
        if changed:
            touched += 1
    return (touched, kept, dropped)

# ===================== UTILS =====================
log  = lambda m: print(m, flush=True)
warn = lambda m: print(f"WARN: {m}", file=sys.stderr, flush=True)
def err(msg: str, code: int = 1): print(f"ERROR: {msg}", file=sys.stderr, flush=True); sys.exit(code)

def now_utc() -> datetime: return datetime.now(timezone.utc)
def now_almaty() -> datetime:
    try:   return datetime.now(ZoneInfo("Asia/Almaty")) if ZoneInfo else datetime.utcfromtimestamp(time.time()+5*3600)
    except Exception: return datetime.utcfromtimestamp(time.time()+5*3600)
def format_dt_almaty(dt: datetime) -> str: return dt.strftime("%d:%m:%Y - %H:%M:%S")
def next_build_time_almaty() -> datetime:
    cur = now_almaty(); t = cur.replace(hour=1, minute=0, second=0, microsecond=0)
    return t + timedelta(days=1) if cur >= t else t

def inner_html(el: ET.Element) -> str:
    if el is None: return ""
    parts=[]
    if el.text: parts.append(el.text)
    for ch in el:
        parts.append(ET.tostring(ch, encoding="unicode"))
        if ch.tail: parts.append(ch.tail)
    return "".join(parts).strip()

def _html_escape_in_cdata_safe(s: str) -> str:
    return (s or "").replace("]]>", "]]&gt;")

def get_text(el: ET.Element, tag: str) -> str:
    node = el.find(tag); return (node.text or "").strip() if node is not None and node.text else ""

def remove_all(el: ET.Element, *tags: str) -> int:
    n=0
    for t in tags:
        for x in list(el.findall(t)): el.remove(x); n+=1
    return n

# ===================== LOAD SOURCE =====================
def load_source_bytes(src: str) -> bytes:
    if not src: raise RuntimeError("SUPPLIER_URL –Ω–µ –∑–∞–¥–∞–Ω")
    if "://" not in src or src.startswith("file://"):
        path = src[7:] if src.startswith("file://") else src
        with open(path, "rb") as f: data=f.read()
        if len(data) < MIN_BYTES: raise RuntimeError(f"file too small: {len(data)}")
        return data
    sess=requests.Session(); headers={"User-Agent":"supplier-feed-bot/1.0 (+github-actions)"}
    last=None
    for i in range(1, RETRIES+1):
        try:
            r=sess.get(src, headers=headers, timeout=TIMEOUT_S)
            if r.status_code!=200: raise RuntimeError(f"HTTP {r.status_code}")
            data=r.content
            if len(data)<MIN_BYTES: raise RuntimeError(f"too small ({len(data)})")
            return data
        except Exception as e:
            last=e; back=RETRY_BACKOFF*i*(1+random.uniform(-0.2,0.2))
            warn(f"fetch {i}/{RETRIES} failed: {e}; sleep {back:.2f}s")
            if i<RETRIES: time.sleep(back)
    raise RuntimeError(f"fetch failed: {last}")

# ===================== NAME FILTER (file + env) =====================
class KeySpec:
    __slots__=("raw","kind","norm","pattern")
    def __init__(self, raw: str, kind: str, norm: Optional[str], pattern: Optional[re.Pattern]):
        self.raw, self.kind, self.norm, self.pattern = raw, kind, norm, pattern

def _norm_name(s: str) -> str:
    s=(s or "").replace("\u00A0"," ").lower().replace("—ë","–µ")
    return re.sub(r"\s+"," ", s).strip()

def load_name_filter(path: str) -> List[KeySpec]:
    if not path or not os.path.exists(path): return []
    data=None
    for enc in ("utf-8-sig","utf-8","utf-16","utf-16-le","utf-16-be","windows-1251"):
        try:
            with open(path,"r",encoding=enc) as f: data=f.read()
            break
        except Exception: continue
    if data is None:
        with open(path,"r",encoding="utf-8",errors="ignore") as f: data=f.read()
    data = data.replace("\ufeff","").replace("\x00","")

    keys: List[KeySpec]=[]
    for ln in data.splitlines():
        s=ln.strip()
        if not s or s.startswith("#"): continue
        if len(s)>=2 and s[0]=="/" and s[-1]=="/":
            try: keys.append(KeySpec(s,"regex",None,re.compile(s[1:-1],re.I)))
            except Exception: pass
        else:
            n=_norm_name(s)
            if n: keys.append(KeySpec(s,"substr",n,None))
    return keys

def name_matches(name: str, keys: List[KeySpec]) -> bool:
    if not keys: return False
    n=_norm_name(name or "")
    for ks in keys:
        if ks.kind=="substr" and (n.startswith(ks.norm) or (ks.norm in n)):
            return True
        if ks.kind=="regex" and ks.pattern and ks.pattern.search(name or ""):
            return True
    return False

# ===================== BRAND / PRICE / AVAIL / ORDER =====================
def _norm_key(s: str) -> str:
    if not s: return ""
    s=s.strip().lower().replace("—ë","–µ")
    s=re.sub(r"[-_/]+"," ",s); s=re.sub(r"\s+"," ",s); return s

SUPPLIER_BLOCKLIST={_norm_key(x) for x in["alstyle","al-style","copyline","akcent","ak-cent","vtt"]}
UNKNOWN_VENDOR_MARKERS=("–Ω–µ–∏–∑–≤–µ—Å—Ç","unknown","–±–µ–∑ –±—Ä–µ–Ω–¥–∞","no brand","noname","no-name","n/a","–∫–∏—Ç–∞–π","china")
COMMON_BRANDS=["Canon","HP","Hewlett-Packard","Xerox","Brother","Epson","BenQ","ViewSonic","Optoma","Acer","Panasonic","Sony",
               "Konica Minolta","Ricoh","Kyocera","Sharp","OKI","Pantum","Lenovo","Dell","ASUS","Samsung","Apple","MSI"]
BRAND_ALIASES={"hewlett packard":"HP","konica":"Konica Minolta","konica-minolta":"Konica Minolta",
               "viewsonic proj":"ViewSonic","epson proj":"Epson","epson projector":"Epson","benq proj":"BenQ",
               "hp inc":"HP","nvprint":"NV Print","nv print":"NV Print","gg":"G&G","g&g":"G&G"}

def normalize_brand(raw: str) -> str:
    k=_norm_key(raw)
    return "" if (not k) or (k in SUPPLIER_BLOCKLIST) else (BRAND_ALIASES.get(k) or raw.strip())

def build_brand_index(shop_el: ET.Element) -> Dict[str,str]:
    idx={}
    off_el=shop_el.find("offers")
    if off_el is None: return idx
    for offer in off_el.findall("offer"):
        v=offer.find("vendor")
        if v is not None and (v.text or "").strip():
            canon=v.text.strip(); idx[_norm_key(canon)] = canon
    return idx

def _find_brand_in_text(text: str) -> str:
    t=(text or "").lower()
    for a,canon in BRAND_ALIASES.items():
        if re.search(rf"\b{re.escape(a)}\b", t): return canon
    for b in COMMON_BRANDS:
        if re.search(rf"\b{re.escape(b.lower())}\b", t): return b
    m=re.match(r"^([A-Za-z–ê-–Ø–∞-—è–Å—ë]+)\b", (text or "").strip())
    if m:
        cand=m.group(1)
        for b in COMMON_BRANDS:
            if b.lower()==cand.lower(): return b
    return ""

def guess_vendor_for_offer(offer: ET.Element, brand_index: Dict[str,str]) -> str:
    name=get_text(offer,"name"); desc=inner_html(offer.find("description"))
    first=(re.split(r"\s+", name.strip())[0] if name else "")
    f_norm=_norm_key(first)
    if f_norm in brand_index: return brand_index[f_norm]
    b = _find_brand_in_text(name) or _find_brand_in_text(desc)
    return b

def ensure_vendor(shop_el: ET.Element) -> Tuple[int,int,int]:
    off_el=shop_el.find("offers")
    if off_el is None: return (0,0,0)
    idx=build_brand_index(shop_el); normalized=0; filled=0; removed=0
    for offer in off_el.findall("offer"):
        ven=offer.find("vendor")
        txt=(ven.text or "").strip() if ven is not None and ven.text else ""
        if txt:
            canon=normalize_brand(txt)
            alias=BRAND_ALIASES.get(_norm_key(txt))
            final=alias or canon
            if any(m in txt.lower() for m in UNKNOWN_VENDOR_MARKERS) or (not final):
                if ven is not None: offer.remove(ven); removed+=1
            elif final!=txt:
                ven.text=final; normalized+=1
        else:
            guess=guess_vendor_for_offer(offer, idx)
            if guess:
                if ven is None: ven=ET.SubElement(offer,"vendor")
                ven.text=guess; filled+=1
    return (normalized, filled, removed)

def parse_price_number(raw:str)->Optional[float]:
    if raw is None: return None
    s=raw.strip().replace("\xa0"," ").replace(" ","").replace("KZT","").replace("kzt","").replace("‚Ç∏","").replace(",",".")
    if not s: return None
    try: v=float(s); return v if v>0 else None
    except Exception: return None

def pick_dealer_price(offer: ET.Element) -> Optional[float]:
    dealer=[]
    for prices in list(offer.findall("prices")) + list(offer.findall("Prices")):
        for p in list(prices.findall("price")) + list(prices.findall("Price")):
            val=parse_price_number(p.text or "")
            if val is None: continue
            t=(p.attrib.get("type") or "")
            if PRICE_KEYWORDS_DEALER.search(t) or not PRICE_KEYWORDS_RRP.search(t):
                dealer.append(val)
    for tag in PRICE_FIELDS_DIRECT:
        el=offer.find(tag)
        if el is not None and el.text:
            v=parse_price_number(el.text)
            if v is not None: dealer.append(v)
    return min(dealer) if dealer else None

_force_tail_900 = lambda n: max(int(n)//1000,0)*1000+900

def compute_retail(d:float,rules:List[PriceRule])->Optional[int]:
    for lo,hi,pct,add in rules:
        if lo<=d<=hi: return _force_tail_900(d*(1+pct/100.0)+add)
    return None

def _remove_all_price_nodes(offer: ET.Element):
    for t in ("price","Price"):
        for node in list(offer.findall(t)): offer.remove(node)

def strip_supplier_price_blocks(offer: ET.Element):
    remove_all(offer,"prices","Prices")
    for tag in INTERNAL_PRICE_TAGS: remove_all(offer, tag)

def reprice_offers(out_shop:ET.Element,rules:List[PriceRule])->None:
    off_el=out_shop.find("offers")
    if off_el is None: return
    for offer in off_el.findall("offer"):
        if offer.attrib.get("_force_price","")=="100":
            strip_supplier_price_blocks(offer); _remove_all_price_nodes(offer)
            ET.SubElement(offer,"price").text=str(PRICE_CAP_VALUE)
            offer.attrib.pop("_force_price",None); continue
        dealer=pick_dealer_price(offer)
        if dealer is None or dealer<=100:
            strip_supplier_price_blocks(offer); continue
        newp=compute_retail(dealer,rules)
        if newp is None:
            strip_supplier_price_blocks(offer); continue
        _remove_all_price_nodes(offer); ET.SubElement(offer,"price").text=str(int(newp)); strip_supplier_price_blocks(offer)

def flag_unrealistic_supplier_prices(out_shop: ET.Element) -> int:
    off_el=out_shop.find("offers")
    if off_el is None: return 0
    flagged=0
    for offer in off_el.findall("offer"):
        try:
            src_p = float((get_text(offer,"price") or "").replace(",",".")) if get_text(offer,"price") else None
        except Exception:
            src_p = None
        if src_p is not None and src_p >= PRICE_CAP_THRESHOLD:
            offer.attrib["_force_price"]=str(PRICE_CAP_VALUE); flagged+=1
    return flagged

TRUE_WORDS={"true","1","yes","y","–¥–∞","–µ—Å—Ç—å","in stock","available"}
FALSE_WORDS={"false","0","no","n","–Ω–µ—Ç","–æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç","–Ω–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏","out of stock","unavailable","–ø–æ–¥ –∑–∞–∫–∞–∑","–æ–∂–∏–¥–∞–µ—Ç—Å—è","–Ω–∞ –∑–∞–∫–∞–∑"}
def _parse_bool_str(s: str)->Optional[bool]:
    v=(s or "").strip().lower()
    return True if v in TRUE_WORDS else False if v in FALSE_WORDS else None

def derive_available(offer: ET.Element) -> bool:
    avail_el=offer.find("available")
    if avail_el is not None and avail_el.text:
        b=_parse_bool_str(avail_el.text)
        if b is not None: return b
    for tag in ["quantity_in_stock","quantity","stock","Stock"]:
        for node in offer.findall(tag):
            try:
                val=int(re.sub(r"[^\d\-]+","", node.text or ""))
                return val>0
            except Exception:
                continue
    for tag in ["status","Status"]:
        node=offer.find(tag)
        if node is not None and node.text:
            b=_parse_bool_str(node.text)
            if b is not None: return b
    return False

def normalize_available_field(out_shop: ET.Element) -> Tuple[int,int]:
    off_el=out_shop.find("offers")
    if off_el is None: return (0,0)
    t=f=0
    for offer in off_el.findall("offer"):
        b=derive_available(offer)
        remove_all(offer,"available")
        offer.attrib["available"]="true" if b else "false"
        if DROP_STOCK_TAGS: remove_all(offer,"quantity_in_stock","quantity","stock","Stock")
        t+=1 if b else 0; f+=0 if b else 1
    return t,f

def fix_currency_id(out_shop: ET.Element, default_code: str = "KZT") -> int:
    off_el=out_shop.find("offers")
    if off_el is None: return 0
    touched=0
    for offer in off_el.findall("offer"):
        remove_all(offer,"currencyId"); ET.SubElement(offer,"currencyId").text=default_code; touched+=1
    return touched

def ensure_categoryid_zero_first(out_shop: ET.Element) -> int:
    off_el=out_shop.find("offers")
    if off_el is None: return 0
    touched=0
    for offer in off_el.findall("offer"):
        remove_all(offer,"categoryId","CategoryId")
        cid=ET.Element("categoryId"); cid.text=os.getenv("CATEGORY_ID_DEFAULT","0")
        offer.insert(0,cid); touched+=1
    return touched

def reorder_offer_children(out_shop: ET.Element) -> int:
    off_el=out_shop.find("offers")
    if off_el is None: return 0
    changed=0
    for offer in off_el.findall("offer"):
        children=list(offer)
        buckets={k:[] for k in DESIRED_ORDER}; others=[]
        for n in children: (buckets[n.tag] if n.tag in buckets else others).append(n)
        rebuilt=[*sum((buckets[k] for k in DESIRED_ORDER), []), *others]
        if rebuilt!=children:
            for n in children: offer.remove(n)
            for n in rebuilt:  offer.append(n)
            changed+=1
    return changed

# ===================== Kind detector =====================
def detect_kind(name: str) -> str:
    n=(name or "").lower()
    if "–∫–∞—Ä—Ç—Ä–∏–¥–∂" in n or "—Ç–æ–Ω–µ—Ä" in n or "—Ç–æ–Ω–µ—Ä-" in n: return "cartridge"
    if "–∏–±–ø" in n or "ups" in n or "–∏—Å—Ç–æ—á–Ω–∏–∫ –±–µ—Å–ø–µ—Ä–µ–±–æ–π–Ω–æ–≥–æ –ø–∏—Ç–∞–Ω–∏—è" in n: return "ups"
    if "–ø—Ä–æ–µ–∫—Ç–æ—Ä" in n or "projector" in n: return "projector"
    if "–ø—Ä–∏–Ω—Ç–µ—Ä" in n or "mfp" in n or "–º—Ñ—É" in n: return "mfp"
    return "other"

# ===================== SEO/FAQ/REVIEWS/COMPAT =====================
AS_INTERNAL_ART_RE = re.compile(r"^AS\d+|^AK\d+|^AC\d+", re.I)
MODEL_RE = re.compile(r"\b([A-Z][A-Z0-9\-]{2,})\b", re.I)
BRAND_WORDS_SEO = ["Canon","HP","Hewlett-Packard","Xerox","Brother","Epson","BenQ","ViewSonic","Optoma","Acer","Panasonic","Sony",
                   "Konica Minolta","Ricoh","Kyocera","Sharp","OKI","Pantum","Lenovo","Dell","ASUS","Samsung","Apple","MSI"]
FAMILY_WORDS = ["PIXMA","imageRUNNER","iR","imageCLASS","imagePRESS","LBP","MF","i-SENSYS","LaserJet","DeskJet","OfficeJet",
                "PageWide","Color LaserJet","Neverstop","Smart Tank","Phaser","WorkCentre","VersaLink","AltaLink","DocuCentre",
                "DCP","HL","MFC","FAX","XP","WF","EcoTank","TASKalfa","ECOSYS","Aficio","SP","MP","IM","MX","BP"]

def split_short_name(name: str) -> str:
    s=(name or "").strip(); s=re.split(r"\s+[‚Äî-]\s+", s, maxsplit=1)[0]
    return s if len(s)<=80 else s[:77]+"..."

def _split_joined_models(s: str) -> List[str]:
    for bw in BRAND_WORDS_SEO:
        s=re.sub(rf"({re.escape(bw)})\s*(?={re.escape(bw)})", r"\1\n", s)
    raw=re.split(r"[,\n;]+", s); return [c.strip() for c in raw if c.strip()]

def _looks_device_phrase(x: str) -> bool:
    if len(x.strip())<3: return False
    has_family=any(re.search(rf"\b{re.escape(f)}\b", x, re.I) for f in FAMILY_WORDS)
    has_brand =any(re.search(rf"\b{re.escape(b)}\b", x, re.I) for b in BRAND_WORDS_SEO)
    has_model=bool(MODEL_RE.search(x) and not AS_INTERNAL_ART_RE.search(x))
    return (has_family or has_brand) and has_model

def extract_full_compatibility(raw_desc: str) -> str:
    t=(raw_desc or "")
    text=re.sub(r"<br\s*/?>","\n",t,flags=re.I); text=re.sub(r"<[^>]+>"," ",text)
    parts=_split_joined_models(text); found=[]
    for sub in parts:
        s=sub.strip()
        if _looks_device_phrase(s): found.append(s)
    clean=[]
    for x in found:
        x=re.sub(r"\s{2,}"," ",x).strip(" ,;.")
        if x and x not in clean: clean.append(x)
    return ", ".join(clean[:50])

def build_lead_faq_reviews(offer: ET.Element) -> Tuple[str,str,str,str]:
    name=get_text(offer,"name"); vendor=get_text(offer,"vendor").strip()
    desc_html=inner_html(offer.find("description"))
    raw_text=re.sub(r"<[^>]+>"," ", re.sub(r"<br\s*/?>","\n",desc_html or "", flags=re.I))
    kind=detect_kind(name)
    s_id=offer.attrib.get("id") or get_text(offer,"vendorCode") or name
    seed=int(hashlib.md5((s_id or "").encode("utf-8")).hexdigest()[:8],16)

    variants={"cartridge":["–ö—Ä–∞—Ç–∫–æ –æ –ø–ª—é—Å–∞—Ö","–ß–µ–º —É–¥–æ–±–µ–Ω","–ß—Ç–æ –ø–æ–ª—É—á–∞–µ—Ç–µ —Å","–î–ª—è –∫–∞–∫–∏—Ö —É—Å—Ç—Ä–æ–π—Å—Ç–≤"],
              "projector":["–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞","–ß–µ–º —Ö–æ—Ä–æ—à","–î–ª—è –∫–∞–∫–∏—Ö –∑–∞–¥–∞—á","–ö—Ä–∞—Ç–∫–æ –æ –ø–ª—é—Å–∞—Ö"],
              "ups":["–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞","–ß–µ–º —É–¥–æ–±–µ–Ω","–ß—Ç–æ –≤—ã –ø–æ–ª—É—á–∞–µ—Ç–µ"],
              "mfp":["–ö—Ä–∞—Ç–∫–æ –æ –ø–ª—é—Å–∞—Ö","–û—Å–Ω–æ–≤–Ω—ã–µ —Å–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã","–î–ª—è –∫–æ–≥–æ –ø–æ–¥–æ–π–¥—ë—Ç"],
              "other":["–ö—Ä–∞—Ç–∫–æ –æ –ø–ª—é—Å–∞—Ö","–ß–µ–º —É–¥–æ–±–µ–Ω","–ö–ª—é—á–µ–≤—ã–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–∞"]}
    short=split_short_name(name)
    p = variants.get(kind, variants["other"])[seed % len(variants.get(kind, variants["other"]))]
    title=f"{short}: {p}" + (f" ({vendor})" if vendor else "")

    bullets=[]
    low=raw_text.lower()
    if kind=="projector":
        if re.search(r"\b(ansi\s*–ª–º|–ª—é–º–µ–Ω|lumen|lm)\b",low): bullets.append("‚úÖ –Ø—Ä–∫–æ—Å—Ç—å: –∑–∞—è–≤–ª–µ–Ω–Ω–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª–µ–º")
        if re.search(r"\b(fhd|1080p|4k|wxga|wuxga|svga|xga|uxga)\b",low): bullets.append("‚úÖ –†–∞–∑—Ä–µ—à–µ–Ω–∏–µ: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –∫–ª–∞—Å—Å—É –º–æ–¥–µ–ª–∏")
        if re.search(r"\b(–∫–æ–Ω—Ç—Ä–∞—Å—Ç|contrast)\b",low): bullets.append("‚úÖ –ö–æ–Ω—Ç—Ä–∞—Å—Ç: –∫–æ–º—Ñ–æ—Ä—Ç–Ω–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –≤ –æ—Ñ–∏—Å–µ/–¥–æ–º–µ")
        bullets.append("‚úÖ –ü–æ–¥—Ö–æ–¥–∏—Ç –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π –∏ –æ–±—É—á–µ–Ω–∏—è")
    elif kind=="cartridge":
        if re.search(r"\b—Ä–µ—Å—É—Ä—Å\b",low): bullets.append("‚úÖ –†–µ—Å—É—Ä—Å: –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º–∞—è –æ—Ç–¥–∞—á–∞ —Å—Ç—Ä–∞–Ω–∏—Ü")
        if re.search(r"\b—Ü–≤–µ—Ç\b|\bcyan|\bmagenta|\byellow|\bblack",low): bullets.append("‚úÖ –¶–≤–µ—Ç–Ω–æ—Å—Ç—å: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Å–ø–µ—Ü–∏—Ñ–∏–∫–∞—Ü–∏–∏")
        bullets.append("‚úÖ –°—Ç–∞–±–∏–ª—å–Ω–∞—è –ø–µ—á–∞—Ç—å –±–µ–∑ –ª–∏—à–Ω–∏—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫")
    elif kind=="ups":
        if re.search(r"\b(–≤–∞|–≤—Ç)\b",low): bullets.append("‚úÖ –ú–æ—â–Ω–æ—Å—Ç—å: —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç–∏–ø–æ–≤—ã–º –æ—Ñ–∏—Å–Ω—ã–º –∑–∞–¥–∞—á–∞–º")
        if re.search(r"\bavr\b|\b—Å—Ç–∞–±–∏–ª–∏–∑",low): bullets.append("‚úÖ AVR/—Å—Ç–∞–±–∏–ª–∏–∑–∞—Ü–∏—è –≤—Ö–æ–¥–Ω–æ–≥–æ –Ω–∞–ø—Ä—è–∂–µ–Ω–∏—è")
        bullets.append("‚úÖ –ë–∞–∑–æ–≤–∞—è –∑–∞—â–∏—Ç–∞ –ü–ö, —Ä–æ—É—Ç–µ—Ä–∞ –∏ –ø–µ—Ä–∏—Ñ–µ—Ä–∏–∏")
    else:
        bullets.append("‚úÖ –ü—Ä–∞–∫—Ç–∏—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã—Ö –∑–∞–¥–∞—á")

    compat = extract_full_compatibility(desc_html) if kind=="cartridge" else ""

    lead=[]
    lead.append(f"<h3>{_html_escape_in_cdata_safe(title)}</h3>")
    p_line={"cartridge":"–°—Ç–∞–±–∏–ª—å–Ω–∞—è –ø–µ—á–∞—Ç—å –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π —Ä–µ—Å—É—Ä—Å.",
            "ups":"–ë–∞–∑–æ–≤–∞—è –∑–∞—â–∏—Ç–∞ –ø–∏—Ç–∞–Ω–∏—è –¥–ª—è –¥–æ–º–∞—à–Ω–µ–π –∏ –æ—Ñ–∏—Å–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏.",
            "projector":"–ß—ë—Ç–∫–∞—è –∫–∞—Ä—Ç–∏–Ω–∫–∞ –∏ –Ω–∞–¥—ë–∂–Ω–∞—è —Ä–∞–±–æ—Ç–∞ –¥–ª—è –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏—è.",
            "mfp":"–°–∫–æ—Ä–æ—Å—Ç—å, —É–¥–æ–±—Å—Ç–≤–æ –∏ –∫–∞—á–µ—Å—Ç–≤–æ –¥–ª—è –æ—Ñ–∏—Å–∞.",
            "other":"–ü—Ä–∞–∫—Ç–∏—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã."}.get(kind,"–ü—Ä–∞–∫—Ç–∏—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã.")
    lead.append(f"<p>{_html_escape_in_cdata_safe(p_line)}</p>")
    if bullets:
        lead.append("<ul>")
        for b in bullets[:5]: lead.append(f"  <li>{_html_escape_in_cdata_safe(b)}</li>")
        lead.append("</ul>")
    if compat:
        compat_html=_html_escape_in_cdata_safe(compat).replace(";", "; ").replace(",", ", ")
        lead.append(f"<p><strong>–ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:</strong><br>{compat_html}</p>")
    lead_html="\n".join(lead)

    if kind=="cartridge":
        qa=[("–ü–æ–¥–æ–π–¥—ë—Ç –∫ –º–æ–µ–º—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É?","–°–≤–µ—Ä—å—Ç–µ –∏–Ω–¥–µ–∫—Å –º–æ–¥–µ–ª–∏ –≤ —Å–ø–∏—Å–∫–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –Ω–∏–∂–µ."),
            ("–ù—É–∂–Ω–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ—Å–ª–µ –∑–∞–º–µ–Ω—ã?","–û–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∏ —Ä–∞—Å–ø–µ—á–∞—Ç–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.")]
    elif kind=="projector":
        qa=[("–ü–æ–¥–æ–π–¥—ë—Ç –¥–ª—è –ø–µ—Ä–µ–≥–æ–≤–æ—Ä–Ω–æ–π?","–î–∞, –¥–ª—è —Ç–∏–ø–æ–≤–æ–π –∫–æ–º–Ω–∞—Ç—ã –∏ –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π/–æ–±—É—á–µ–Ω–∏—è."),
            ("–ù—É–∂–Ω–æ –∑–∞—Ç–µ–º–Ω–µ–Ω–∏–µ?","–ü—Ä–∏ –≤—ã—Å–æ–∫–æ–π —è—Ä–∫–æ—Å—Ç–∏ –ª—É—á—à–µ –ø—Ä–∏–≥–ª—É—à–∏—Ç—å —Å–≤–µ—Ç –¥–ª—è –∫–æ–Ω—Ç—Ä–∞—Å—Ç–∞.")]
    elif kind=="ups":
        qa=[("–ü–æ–¥–æ–π–¥—ë—Ç –¥–ª—è –ü–ö –∏ —Ä–æ—É—Ç–µ—Ä–∞?","–î–∞, –¥–ª—è —Ç–µ—Ö–Ω–∏–∫–∏ —Å–≤–æ–µ–≥–æ –∫–ª–∞—Å—Å–∞ –º–æ—â–Ω–æ—Å—Ç–∏."),
            ("–®—É–º–∏—Ç –ª–∏ –≤ —Ä–∞–±–æ—Ç–µ?","–í –æ–±—ã—á–Ω–æ–º —Ä–µ–∂–∏–º–µ ‚Äî —Ç–∏—Ö–æ; —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Å–æ–±—ã—Ç–∏—è—Ö.")]
    else:
        qa=[("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏?","–î–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—É—é —Ä–∞–±–æ—Ç—É."),
            ("–ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏?","–î–∞, –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ ‚Äî –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö –º–æ–¥–µ–ª–∏.")]
    faq=["<h3>FAQ</h3>"]+[f"<p><strong>–í:</strong> { _html_escape_in_cdata_safe(q) }<br><strong>–û:</strong> { _html_escape_in_cdata_safe(a) }</p>" for q,a in qa]
    faq_html="\n".join(faq)

    # –§–∏–∫—Ç–∏–≤–Ω—ã–µ –æ—Ç–∑—ã–≤—ã (–ª–æ–∫–∞–ª–∏–∑–æ–≤–∞–Ω–æ)
    NAMES_M=["–ê—Ä–º–∞–Ω","–î–∞—É—Ä–µ–Ω","–°–∞–Ω–∂–∞—Ä","–ï—Ä–ª–∞–Ω","–ê—Å–ª–∞–Ω","–†—É—Å–ª–∞–Ω","–¢–∏–º—É—Ä","–î–∞–Ω–∏—è—Ä","–í–∏–∫—Ç–æ—Ä","–ï–≤–≥–µ–Ω–∏–π","–û–ª–µ–≥","–°–µ—Ä–≥–µ–π","–ù—É—Ä–∂–∞–Ω","–ë–µ–∫–∑–∞—Ç","–ê–∑–∞–º–∞—Ç","–°—É–ª—Ç–∞–Ω"]
    NAMES_F=["–ê–π–≥–µ—Ä–∏–º","–ú–∞—Ä–∏—è","–ò–Ω–Ω–∞","–ù–∞—Ç–∞–ª—å—è","–ñ–∞–Ω–Ω–∞","–°–≤–µ—Ç–ª–∞–Ω–∞","–û–ª—å–≥–∞","–ö–∞–º–∏–ª–ª–∞","–î–∏–∞–Ω–∞","–ì—É–ª—å–Ω–∞—Ä–∞"]
    CITIES=["–ê–ª–º–∞—Ç—ã","–ê—Å—Ç–∞–Ω–∞","–®—ã–º–∫–µ–Ω—Ç","–ö–∞—Ä–∞–≥–∞–Ω–¥–∞","–ê–∫—Ç–æ–±–µ","–ü–∞–≤–ª–æ–¥–∞—Ä","–ê—Ç—ã—Ä–∞—É","–¢–∞—Ä–∞–∑","–û—Å–∫–µ–º–µ–Ω","–°–µ–º–µ–π","–ö–æ—Å—Ç–∞–Ω–∞–π","–ö—ã–∑—ã–ª–æ—Ä–¥–∞","–û—Ä–∞–ª","–ü–µ—Ç—Ä–æ–ø–∞–≤–ª","–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω","–ê–∫—Ç–∞—É","–¢–µ–º–∏—Ä—Ç–∞—É","–≠–∫–∏–±–∞—Å—Ç—É–∑","–ö–æ–∫—à–µ—Ç–∞—É","–†—É–¥–Ω—ã–π"]
    pick=lambda arr,offs=0: arr[(seed+offs)%len(arr)]
    reviews=["<h3>–û—Ç–∑—ã–≤—ã (3)</h3>"]
    rv=[("‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê","–ö–∞—Ä—Ç–∏–Ω–∫–∞ —á—ë—Ç–∫–∞—è, –¥–ª—è –ø—Ä–µ–∑–µ–Ω—Ç–∞—Ü–∏–π ‚Äî —Ç–æ, —á—Ç–æ –Ω–∞–¥–æ."),
        ("‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê","–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–Ω—è–ª–∞ –ø–∞—Ä—É –º–∏–Ω—É—Ç, –ø—Ä–æ–±–ª–µ–º –Ω–µ –±—ã–ª–æ."),
        ("‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ","–°–æ —Å–≤–æ–∏–º–∏ –∑–∞–¥–∞—á–∞–º–∏ —Å–ø—Ä–∞–≤–ª—è–µ—Ç—Å—è –æ—Ç–ª–∏—á–Ω–æ.")]
    for i,(stars,comment) in enumerate(rv):
        name=(pick(NAMES_M,i) if i!=1 else pick(NAMES_F,i))
        city=pick(CITIES,i+3)
        reviews.append(f"<p>üë§ <strong>{_html_escape_in_cdata_safe(name)}</strong>, { _html_escape_in_cdata_safe(city) } ‚Äî {stars}<br>¬´{ _html_escape_in_cdata_safe(comment) }¬ª</p>")
    reviews_html="\n".join(reviews)
    return lead_html, faq_html, reviews_html, kind

# ===================== ¬´–†–æ–¥–Ω–æ–π¬ª —Ç–µ–∫—Å—Ç ‚Üí –•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏ (–ü–ê–¢–ß –ü–û –°–ï–ö–¶–ò–Ø–ú) =====================
KV_KEYS_MAP = {
    "–≤–∏–¥":"–í–∏–¥",
    "–Ω–∞–∑–Ω–∞—á–µ–Ω–∏–µ":"–ù–∞–∑–Ω–∞—á–µ–Ω–∏–µ",
    "—Ü–≤–µ—Ç –ø–µ—á–∞—Ç–∏":"–¶–≤–µ—Ç –ø–µ—á–∞—Ç–∏",
    "–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã–µ –º–æ–¥–µ–ª–∏ –ø—Ä–∏–Ω—Ç–µ—Ä–æ–≤":"–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å",
    "—Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å":"–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å",
    "—Ä–µ—Å—É—Ä—Å":"–†–µ—Å—É—Ä—Å",
    "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –ø–µ—á–∞—Ç–∏":"–¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è –ø–µ—á–∞—Ç–∏",
    "—Ç–∏–ø":"–¢–∏–ø",
}

# --- PATCH: —Å–ø–∏—Å–æ–∫ —è–≤–Ω—ã—Ö —Å–µ–∫—Ü–∏–π, –ø–æ—Å–ª–µ –∫–æ—Ç–æ—Ä—ã—Ö –Ω—É–∂–Ω–æ –æ—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞—Ç—å—Å—è ---
SECTION_BREAKERS = {
    "–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å—ã","–∞—É–¥–∏–æ","–ø–∏—Ç–∞–Ω–∏–µ","–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ","—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ",
    "—É—Å–ª–æ–≤–∏—è —ç–∫—Å–ø–ª—É–∞—Ç–∞—Ü–∏–∏","–∫—Ä–µ–ø–ª–µ–Ω–∏–µ","–≤—Ö–æ–¥–Ω–æ–π —Å–∏–≥–Ω–∞–ª","–≤–∏–¥–µ–æ –≤—Ö–æ–¥—ã",
    "—ç—Ä–≥–æ–Ω–æ–º–∏–∫–∞","–≤–µ—Å (—Ñ—É–Ω—Ç—ã)","–≤–µ—Å (–∫–≥)","—Ä–∞–∑–º–µ—Ä—ã (–¥—é–π–º—ã)","—Ä–∞–∑–º–µ—Ä—ã (–º–º)",
    "–æ–±—â–µ–µ","—Å–µ—Ä—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è","—Å–æ—Å—Ç–∞–≤ —É–ø–∞–∫–æ–≤–∫–∏","–ø–µ—Ä–µ—Ä–∞–±–æ—Ç–∫–∞"
}

def _is_section_header(s: str) -> bool:
    """–ó–∞–≥–æ–ª–æ–≤–æ–∫ —Å–µ–∫—Ü–∏–∏ –±–µ–∑ –¥–≤–æ–µ—Ç–æ—á–∏—è (–∏–ª–∏ –∏–∑ –∏–∑–≤–µ—Å—Ç–Ω—ã—Ö), —á—Ç–æ–±—ã –Ω–µ '–ø—Ä–æ–≥–ª–∞—Ç—ã–≤–∞—Ç—å' –∏—Ö –≤ –ø—Ä–µ–¥—ã–¥—É—â—É—é —Å–µ–∫—Ü–∏—é."""
    t = (s or "").strip()
    if not t: return False
    t_norm = t.strip(":").lower().replace("—ë","–µ")
    if t_norm in SECTION_BREAKERS: return True
    if t_norm in KV_KEYS_MAP: return True
    # –≠–≤—Ä–∏—Å—Ç–∏–∫–∞: 1‚Äì2 —Å–ª–æ–≤–∞, –±–µ–∑ —Ü–∏—Ñ—Ä, –±–µ–∑ ':', –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è —Å –±—É–∫–≤—ã
    if ":" not in t and re.match(r"^[A-Za-z–ê-–Ø–∞-—è–Å—ë]+(?: [A-Za-z–ê-–Ø–∞-—è–Å—ë]+)?$", t) and not re.search(r"\d", t):
        return True
    return False

MORE_PHRASES_RE = re.compile(r"^\s*(–ø–æ–¥—Ä–æ–±–Ω–µ–µ|—á–∏—Ç–∞—Ç—å –¥–∞–ª–µ–µ|—É–∑–Ω–∞—Ç—å –±–æ–ª—å—à–µ|–≤—Å–µ –¥–µ—Ç–∞–ª–∏|–ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏|—Å–º–æ—Ç—Ä–µ—Ç—å –Ω–∞ —Å–∞–π—Ç–µ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—è|—Å–∫–∞—á–∞—Ç—å –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é)\s*\.?\s*$", re.I)
URL_RE = re.compile(r"https?://\S+", re.I)

def autocorrect_minor_typos_in_html(html: str) -> str:
    s = html or ""
    s = re.sub(r"\b–≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Ç—Å–≤–µ–Ω–Ω—É—é\b", "–≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—É—é", s, flags=re.I)
    s = re.sub(r"\b–ø—Ä–∏–µ–Ω—Ç–µ—Ä–æ–≤\b", "–ø—Ä–∏–Ω—Ç–µ—Ä–æ–≤", s, flags=re.I)
    s = re.sub(r"\bSC-\s*P(\d{3,4}\b)", r"SC-P\1", s)
    s = re.sub(r"SureColor\s+SC-\s*P", "SureColor SC-P", s)
    s = re.sub(r"(\d)\s*–º–ª\b", r"\1 –º–ª", s, flags=re.I)
    s = re.sub(r"[ ]{2,}", " ", s)
    return s

def _html_to_text(desc_html: str) -> str:
    t = re.sub(r"<br\s*/?>", "\n", desc_html or "", flags=re.I)
    t = re.sub(r"</p\s*>", "\n", t, flags=re.I)
    t = re.sub(r"<a\b[^>]*>.*?</a>", "", t, flags=re.I|re.S)
    t = re.sub(r"<[^>]+>", " ", t)
    t = t.replace("\u00A0"," ")
    t = re.sub(r"[ \t]+\n", "\n", t)
    t = re.sub(r"\n{3,}", "\n\n", t)
    return t.strip()

def _normalize_models_list(val: str) -> str:
    x = val or ""
    x = re.sub(r"\bSC-\s*P(\d{3,4}\b)", r"SC-P\1", x)
    x = re.sub(r"\s{2,}", " ", x)
    parts = re.split(r"[,\n;]+", x)
    parts = [p.strip(" .") for p in parts if p.strip()]
    seen=set(); out=[]
    for p in parts:
        if p not in seen:
            seen.add(p); out.append(p)
    return "; ".join(out)

TECH_KEYWORDS = [
    "—Ç–∏–ø","–º–æ–¥–µ–ª—å","—Å–µ—Ä–∏—è","—Å–æ–≤–º–µ—Å—Ç–∏–º","—Ä–µ—Å—É—Ä—Å","—Ü–≤–µ—Ç –ø–µ—á–∞—Ç–∏","—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –ø–µ—á–∞—Ç–∏","—Å–∫–æ—Ä–æ—Å—Ç—å –ø–µ—á–∞—Ç–∏",
    "—Ä–∞–∑—Ä–µ—à–µ–Ω–∏–µ","—Ñ–æ—Ä–º–∞—Ç –±—É–º–∞–≥–∏","–ª–æ—Ç–æ–∫","–∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å","–ø–æ—Ä—Ç","usb","ethernet","wi-fi","bluetooth","lan",
    "–¥–∏–∞–≥–æ–Ω–∞–ª","—è—Ä–∫–æ—Å—Ç","–∫–æ–Ω—Ç—Ä–∞—Å—Ç","–≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫","—Ç–∏–ø –º–∞—Ç—Ä–∏—Ü—ã","—á–∞—Å—Ç–æ—Ç–∞ –æ–±–Ω–æ–≤–ª–µ–Ω","—É–≥–æ–ª –æ–±–∑–æ—Ä–∞","hdr",
    "—Ä–∞–∑—ä–µ–º","—Ä–∞–∑—ä—ë–º","–¥–ª–∏–Ω–∞ –∫–∞–±–µ–ª—è","–º–∞—Ç–µ—Ä–∏–∞–ª","–µ–º–∫–æ—Å—Ç—å","–æ–±—ä–µ–º","—Ñ–æ—Ä–º-—Ñ–∞–∫—Ç–æ—Ä","—Ç–∏–ø –ø–∞–º—è—Ç–∏",
    "—Å–∫–æ—Ä–æ—Å—Ç—å —á—Ç–µ–Ω–∏—è","—Å–∫–æ—Ä–æ—Å—Ç—å –∑–∞–ø–∏—Å–∏","–º–æ—â–Ω–æ—Å—Ç","–Ω–∞–ø—Ä—è–∂–µ–Ω","—á–∞—Å—Ç–æ—Ç","—Å–∏–ª–∞ —Ç–æ–∫–∞","—ç–Ω–µ—Ä–≥–æ–ø–æ—Ç—Ä–µ–±–ª–µ–Ω",
    "–≤–µ—Å","–≥–∞–±–∞—Ä–∏—Ç","—Ä–∞–∑–º–µ—Ä","—Å—Ç—Ä–∞–Ω–∞","–≥–∞—Ä–∞–Ω—Ç–∏","–∫–æ–º–ø–ª–µ–∫—Ç–∞—Ü","dpi","–¥—é–π–º","–º–º","—Å–º","–∫–≥","–≤—Ç","–≤","–≥—Ü","–ª–º","¬∞c"
]
TECH_KEYWORDS_RE = re.compile("|".join([re.escape(k) for k in TECH_KEYWORDS]), re.I)
UNITS_RE = re.compile(r"\b(\d+[.,]?\d*\s?(–º–º|—Å–º|–º|–∫–≥|–≥|–í—Ç|–í|–ì—Ü|–º–ê—á|–ê—á|dpi|–ª–º|–ì–ë|–ú–ë|TB|Hz|V|W|A|VA|dB|¬∞C|\"|–¥—é–π–º))\b", re.I)
BRAND_WORDS = {"canon","hp","hewlett-packard","xerox","brother","epson","benq","viewsonic","optoma","acer",
               "panasonic","sony","konica minolta","ricoh","kyocera","sharp","oki","pantum","lenovo","dell","asus","samsung","apple","msi"}
STOP_KEYS = {"–¥–ª—è","–∏
