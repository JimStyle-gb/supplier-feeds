# scripts/build_alstyle.py
# -*- coding: utf-8 -*-
"""
AlStyle ‚Üí YML: —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ —Ü–µ–Ω—ã/–Ω–∞–ª–∏—á–∏–µ + –±–µ–∑–æ–ø–∞—Å–Ω—ã–π HTML –¥–ª—è <description>.
–ì–ª–∞–≤–Ω–æ–µ:
- ¬´–†–æ–¥–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ¬ª –æ—Å—Ç–∞–≤–ª—è–µ–º –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π.
- –°—Ç—Ä—É–∫—Ç—É—Ä–∞ <description>: [SEO-–±–ª–æ–∫ (–±–µ–∑ FAQ/–æ—Ç–∑—ã–≤–æ–≤)] + [—Ä–æ–¥–Ω–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ] + [–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏*] + [FAQ] + [–û—Ç–∑—ã–≤—ã].
  *¬´–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏¬ª –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–æ–±–∞–≤–ª—è—é—Ç—Å—è –∏–∑ <param>, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ ¬´—Ä–æ–¥–Ω–æ–º¬ª –æ–ø–∏—Å–∞–Ω–∏–∏.
- –î–ª—è –∫–∞—Ä—Ç—Ä–∏–¥–∂–µ–π ‚Äî –≤ SEO-–±–ª–æ–∫–µ –ø–æ–ª–Ω—ã–π —Å–ø–∏—Å–æ–∫ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏.
- –õ–∏–ø–∫–∏–π SEO (sticky): –∫—ç—à docs/alstyle_cache/seo_cache.json (–¥–µ—Ç–µ—Ä–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–æ offer id).
- FEED_META: –¥–æ–±–∞–≤–ª–µ–Ω–∞ —Å—Ç—Ä–æ–∫–∞ ¬´–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ SEO-–±–ª–æ–∫–∞¬ª, –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –≤—Ä–µ–º–µ–Ω–∏; –≤—Å–µ '|' –≤—ã—Ä–æ–≤–Ω–µ–Ω—ã.

ENV:
  SUPPLIER_URL, OUT_FILE, OUTPUT_ENCODING, TIMEOUT_S, RETRIES, RETRY_BACKOFF_S
  PRICE_CAP_THRESHOLD, PRICE_CAP_VALUE
  VENDORCODE_PREFIX, VENDORCODE_CREATE_IF_MISSING
  ALSTYLE_CATEGORIES_PATH, ALSTYLE_CATEGORIES_MODE
  SATU_KEYWORDS, SATU_KEYWORDS_MAXLEN, SATU_KEYWORDS_MAXWORDS, SATU_KEYWORDS_GEO, SATU_KEYWORDS_GEO_MAX, SATU_KEYWORDS_GEO_LAT
  SEO_STICKY=1|0, SEO_CACHE_PATH=docs/alstyle_cache/seo_cache.json, SEO_REFRESH_DAYS=14
"""

from __future__ import annotations
import os, sys, re, time, random, json, hashlib, urllib.parse, requests
from copy import deepcopy
from typing import Dict, List, Tuple, Optional, Set
from xml.etree import ElementTree as ET
from datetime import datetime, timezone, timedelta
from html import unescape as _unescape
try:
    from zoneinfo import ZoneInfo
except Exception:
    ZoneInfo = None

SCRIPT_VERSION = "alstyle-2025-10-21.SEOblock-sticky-safehtml.v3"

# ========== ENV / CONST ==========
SUPPLIER_NAME = os.getenv("SUPPLIER_NAME", "AlStyle").strip()
SUPPLIER_URL  = os.getenv("SUPPLIER_URL", "https://al-style.kz/upload/catalog_export/al_style_catalog.php").strip()
OUT_FILE_YML  = os.getenv("OUT_FILE", "docs/alstyle.yml").strip()
ENC           = os.getenv("OUTPUT_ENCODING", "windows-1251").strip()
TIMEOUT_S     = int(os.getenv("TIMEOUT_S", "30"))
RETRIES       = int(os.getenv("RETRIES", "4"))
RETRY_BACKOFF = float(os.getenv("RETRY_BACKOFF_S", "2"))
MIN_BYTES     = int(os.getenv("MIN_BYTES", "1500"))
DRY_RUN       = os.getenv("DRY_RUN", "0").lower() in {"1","true","yes"}

ALSTYLE_CATEGORIES_PATH = os.getenv("ALSTYLE_CATEGORIES_PATH", "docs/alstyle_categories.txt")
ALSTYLE_CATEGORIES_MODE = os.getenv("ALSTYLE_CATEGORIES_MODE", "off").lower()  # off|include|exclude

# PRICE CAP
PRICE_CAP_THRESHOLD = int(os.getenv("PRICE_CAP_THRESHOLD", "9999999"))
PRICE_CAP_VALUE     = int(os.getenv("PRICE_CAP_VALUE", "100"))

# KEYWORDS
SATU_KEYWORDS          = os.getenv("SATU_KEYWORDS", "auto").lower()
SATU_KEYWORDS_MAXLEN   = int(os.getenv("SATU_KEYWORDS_MAXLEN", "1024"))
SATU_KEYWORDS_MAXWORDS = int(os.getenv("SATU_KEYWORDS_MAXWORDS", "1000"))
SATU_KEYWORDS_GEO      = os.getenv("SATU_KEYWORDS_GEO", "on").lower() in {"on","1","true","yes"}
SATU_KEYWORDS_GEO_MAX  = int(os.getenv("SATU_KEYWORDS_GEO_MAX", "20"))
SATU_KEYWORDS_GEO_LAT  = os.getenv("SATU_KEYWORDS_GEO_LAT", "on").lower() in {"on","1","true","yes"}

# SEO sticky cache (–Ω–æ–≤—ã–π –ø—É—Ç—å)
DEFAULT_CACHE_PATH = "docs/alstyle_cache/seo_cache.json"
SEO_CACHE_PATH     = os.getenv("SEO_CACHE_PATH", DEFAULT_CACHE_PATH)
SEO_STICKY         = os.getenv("SEO_STICKY", "1").lower() in {"1","true","yes","on"}
SEO_REFRESH_DAYS   = int(os.getenv("SEO_REFRESH_DAYS", "14"))
LEGACY_CACHE_PATH  = "docs/seo_cache.json"

# Purge internals
DROP_CATEGORY_ID_TAG   = True
DROP_STOCK_TAGS        = True
PURGE_TAGS_AFTER       = ("Offer_ID","delivery","local_delivery_cost","manufacturer_warranty","model","url","status","Status")
PURGE_OFFER_ATTRS_AFTER= ("type","article")
INTERNAL_PRICE_TAGS    = ("purchase_price","purchasePrice","wholesale_price","wholesalePrice","opt_price","optPrice",
                          "b2b_price","b2bPrice","supplier_price","supplierPrice","min_price","minPrice",
                          "max_price","maxPrice","oldprice")

# ========== UTILS ==========
log  = lambda m: print(m, flush=True)
warn = lambda m: print(f"WARN: {m}", file=sys.stderr, flush=True)
def err(msg: str, code: int = 1): print(f"ERROR: {msg}", file=sys.stderr, flush=True); sys.exit(code)

def now_utc() -> datetime: return datetime.now(timezone.utc)
def now_utc_str() -> str: return now_utc().strftime("%Y-%m-%d %H:%M:%S %Z")
def now_almaty() -> datetime:
    try:   return datetime.now(ZoneInfo("Asia/Almaty")) if ZoneInfo else datetime.utcfromtimestamp(time.time()+5*3600)
    except Exception: return datetime.utcfromtimestamp(time.time()+5*3600)
def format_dt_almaty(dt: datetime) -> str: return dt.strftime("%d:%m:%Y - %H:%M:%S")  # –ª–∞—Ç–∏–Ω—Å–∫–∞—è M
def next_build_time_almaty() -> datetime:
    cur = now_almaty(); t = cur.replace(hour=1, minute=0, second=0, microsecond=0)
    return t + timedelta(days=1) if cur >= t else t

_COLON_CLASS_RE = re.compile("[:\uFF1A\uFE55\u2236\uFE30]")
canon_colons    = lambda s: _COLON_CLASS_RE.sub(":", s or "")
NOISE_RE = re.compile(r"[\u200B-\u200F\u202A-\u202E\u2060\uFEFF\u00AD\u0000-\u0008\u000B\u000C\u000E-\u001F\u007F\u0080-\u009F]")
def strip_noise_chars(s: str) -> str:
    if not s: return ""
    return NOISE_RE.sub("", s).replace("ÔøΩ","")

def _html_escape_in_cdata_safe(s: str) -> str:
    return (s or "").replace("]]>", "]]&gt;")

# ========== LOAD SOURCE ==========
def load_source_bytes(src: str) -> bytes:
    if not src: raise RuntimeError("SUPPLIER_URL –Ω–µ –∑–∞–¥–∞–Ω")
    if "://" not in src or src.startswith("file://"):
        path = src[7:] if src.startswith("file://") else src
        with open(path, "rb") as f: data=f.read()
        if len(data) < MIN_BYTES: raise RuntimeError(f"file too small: {len(data)}")
        return data
    sess=requests.Session(); headers={"User-Agent":"supplier-feed-bot/1.0 (+github-actions)"}
    last=None
    for i in range(1, RETRIES+1):
        try:
            r=sess.get(src, headers=headers, timeout=TIMEOUT_S)
            if r.status_code!=200: raise RuntimeError(f"HTTP {r.status_code}")
            data=r.content
            if len(data)<MIN_BYTES: raise RuntimeError(f"too small ({len(data)})")
            return data
        except Exception as e:
            last=e; back=RETRY_BACKOFF*i*(1+random.uniform(-0.2,0.2))
            warn(f"fetch {i}/{RETRIES} failed: {e}; sleep {back:.2f}s")
            if i<RETRIES: time.sleep(back)
    raise RuntimeError(f"fetch failed: {last}")

# ========== XML HELPERS ==========
def get_text(el: ET.Element, tag: str) -> str:
    node = el.find(tag)
    return (node.text or "").strip() if node is not None and node.text else ""

def remove_all(el: ET.Element, *tags: str) -> int:
    n=0
    for t in tags:
        for x in list(el.findall(t)):
            el.remove(x); n+=1
    return n

def _remove_all_price_nodes(offer: ET.Element):
    for t in ("price", "Price"):
        for node in list(offer.findall(t)): offer.remove(node)

def strip_supplier_price_blocks(offer: ET.Element):
    remove_all(offer, "prices", "Prices")
    for tag in INTERNAL_PRICE_TAGS: remove_all(offer, tag)

# ========== CATEGORY TREE, BRAND, IDS (–±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π –ø–æ —Å—É—Ç–∏) ==========
# ... (–æ—Å—Ç–∞–≤–∏–ª –∫–∞–∫ –≤ v2; —Å–º. –¥–∞–ª—å–Ω–µ–π—à–∏–π –∫–æ–¥ ‚Äî –≤—Å—ë –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç) ...

# ========== PARAMS / TEXT PARSING ==========
EXCLUDE_NAME_RE = re.compile(r"(?:\b–∞—Ä—Ç–∏–∫—É–ª\b|–±–ª–∞–≥–æ—Ç–≤–æ—Ä–∏—Ç–µ–ª—å–Ω\w*|—à—Ç—Ä–∏—Ö–∫–æ–¥|–æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω\w*\s*–∫–æ–¥|–Ω–æ–≤–∏–Ω–∫\w*|—Å–Ω–∏–∂–µ–Ω–∞\s*—Ü–µ–Ω–∞|–∫–æ–¥\s*—Ç–Ω\s*–≤—ç–¥(?:\s*eaeu)?|–∫–æ–¥\s*—Ç–Ω–≤—ç–¥(?:\s*eaeu)?|—Ç–Ω\s*–≤—ç–¥|—Ç–Ω–≤—ç–¥|tn\s*ved|hs\s*code)", re.I)

def remove_specific_params(shop_el: ET.Element) -> int:
    offers_el=shop_el.find("offers")
    if offers_el is None: return 0
    removed=0
    for offer in offers_el.findall("offer"):
        seen=set()
        for tag in ("param","Param"):
            for p in list(offer.findall(tag)):
                nm=(p.attrib.get("name") or "").strip(); val=(p.text or "").strip()
                if not nm or not val: offer.remove(p); removed+=1; continue
                if EXCLUDE_NAME_RE.search(nm): offer.remove(p); removed+=1; continue
                k=nm.strip().lower()
                if k in seen: offer.remove(p); removed+=1; continue
                seen.add(k)
    return removed

# --- –í–´–¢–Ø–ì–ò–í–ê–ï–ú KV –ò –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨ (READ-ONLY) ---
HDR_RE = re.compile(r"^\s*(—Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–µ\s+—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏|—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏)\s*:?\s*$", re.I)
HEAD_ONLY_RE = re.compile(r"^\s*(?:–æ—Å–Ω–æ–≤–Ω—ã–µ\s+)?—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏\s*[:ÔºöÔπï‚à∂Ô∏∞-]*\s*$", re.I)
HEAD_PREFIX_RE = re.compile(r"^\s*(?:–æ—Å–Ω–æ–≤–Ω—ã–µ\s+)?—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏\s*[:ÔºöÔπï‚à∂Ô∏∞-]*\s*", re.I)
KV_COLON_RE  = re.compile(r"^\s*([^:]{2,}?)\s*:\s*(.+)$")
URL_RE       = re.compile(r"https?://\S+", re.I)

def canon_colons(s: str) -> str: return _COLON_CLASS_RE.sub(":", s or "")
def normalize_free_text_punct(s: str) -> str:
    t=canon_colons(s or ""); t=re.sub(r":\s*:", ": ", t); t=re.sub(r"(?<!\.)\.\.(?!\.)", ".", t)
    return re.sub(r"\s{2,}", " ", t).strip()

def extract_kv_from_description(text: str) -> List[Tuple[str,str]]:
    if not (text or "").strip(): return []
    t=(text or "").replace("\r\n","\n").replace("\r","\n")
    lines=[ln.strip() for ln in t.split("\n") if ln.strip()]
    pairs=[]
    for ln in lines:
        if HDR_RE.match(ln) or HEAD_ONLY_RE.match(ln): continue
        ln=HEAD_PREFIX_RE.sub("", ln)
        if URL_RE.search(ln) and ":" not in ln: continue
        m=KV_COLON_RE.match(canon_colons(ln))
        if m:
            name=(m.group(1) or "").strip()
            val=(m.group(2) or "").strip()
            if name and val: pairs.append((name, normalize_free_text_punct(val)))
    return pairs

def build_specs_pairs_from_params(offer: ET.Element) -> List[Tuple[str,str]]:
    pairs=[]; seen=set()
    for p in list(offer.findall("param")) + list(offer.findall("Param")):
        raw_name=(p.attrib.get("name") or "").strip(); raw_val =(p.text or "").strip()
        if not raw_name or not raw_val or EXCLUDE_NAME_RE.search(raw_name): continue
        k=raw_name.strip().lower()
        if k in seen: continue
        seen.add(k); pairs.append((raw_name.strip(), normalize_free_text_punct(raw_val)))
    return pairs

def extract_full_compatibility(raw_desc: str, params_pairs: List[Tuple[str,str]]) -> str:
    for n,v in params_pairs:
        if n.strip().lower().startswith("—Å–æ–≤–º–µ—Å—Ç–∏–º"): return v.strip()
    for n,v in extract_kv_from_description(raw_desc or ""):
        if n.strip().lower().startswith("—Å–æ–≤–º–µ—Å—Ç–∏–º"): return v.strip()
    return ""

# ========== –î–û–ë–ê–í–õ–ï–ù –ù–û–í–´–ô –ë–õ–û–ö: ¬´–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏¬ª –ò–ó PARAM ==========
SPEC_PREFERRED_ORDER = [
    "–º–æ—â–Ω–æ—Å—Ç—å", "—ë–º–∫–æ—Å—Ç—å –±–∞—Ç–∞—Ä–µ–∏", "–µ–º–∫–æ—Å—Ç—å –±–∞—Ç–∞—Ä–µ–∏", "–≤—Ä–µ–º—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤", "–¥–∏–∞–ø–∞–∑–æ–Ω —Ä–∞–±–æ—Ç—ã avr",
    "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ —Ç–∏–ø –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ä–∞–∑—ä—ë–º–æ–≤", "–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ —Ç–∏–ø –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ä–∞–∑—ä–µ–º–æ–≤",
    "—Ñ–æ—Ä–º–∞ –≤—ã—Ö–æ–¥–Ω–æ–≥–æ —Å–∏–≥–Ω–∞–ª–∞", "–≤—ã—Ö–æ–¥–Ω–∞—è —á–∞—Å—Ç–æ—Ç–∞", "–≥–∞–±–∞—Ä–∏—Ç—ã (—à—Ö–≥—Ö–≤)",
    "–≤–µ—Å", "–¥–ª–∏–Ω–∞ –∫–∞–±–µ–ª—è", "–∑–∞—â–∏—Ç–∞ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–π –ª–∏–Ω–∏–∏", "–∑–∞—â–∏—Ç–∞ –æ—Ç –ø–æ–ª–Ω–æ–≥–æ —Ä–∞–∑—Ä—è–¥–∞ –±–∞—Ç–∞—Ä–µ–∏", "–±–µ—Å—à—É–º–Ω—ã–π —Ä–µ–∂–∏–º",
    "—Ü–≤–µ—Ç", "–≥–∞—Ä–∞–Ω—Ç–∏—è", "—Å–æ—Å—Ç–∞–≤", "—Ä–∞–±–æ—á–∏–π –¥–∏–∞–ø–∞–∑–æ–Ω —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä", "—Ä–∞–±–æ—á–∞—è –≤–ª–∞–∂–Ω–æ—Å—Ç—å", "–ª–∏—Ü–µ–≤–∞—è –ø–∞–Ω–µ–ª—å"
]

def _rank_key(k: str) -> Tuple[int, str]:
    k_low = k.strip().lower()
    for i, pref in enumerate(SPEC_PREFERRED_ORDER):
        if k_low == pref: return (i, k)
    # –º—è–≥–∫–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –Ω–∞—á–∞–ª—É —Å–ª–æ–≤–∞
    for i, pref in enumerate(SPEC_PREFERRED_ORDER):
        if k_low.startswith(pref): return (i, k)
    return (1000, k_low)

def has_specs_in_raw_desc(raw_desc_html: str) -> bool:
    if not raw_desc_html: return False
    s = raw_desc_html.lower()
    return ("<ul" in s and "<li" in s) or ("—Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫" in s)

def build_specs_html_from_params(offer: ET.Element) -> str:
    pairs = build_specs_pairs_from_params(offer)
    if not pairs: return ""
    # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è–º, –∑–∞—Ç–µ–º –∞–ª—Ñ–∞–≤–∏—Ç—É
    pairs_sorted = sorted(pairs, key=lambda kv: _rank_key(kv[0]))
    parts = ["<h3>–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏</h3>", "<ul>"]
    for name, val in pairs_sorted:
        parts.append(f"  <li><strong>{_html_escape_in_cdata_safe(name)}:</strong> { _html_escape_in_cdata_safe(val) }</li>")
    parts.append("</ul>")
    return "\n".join(parts)

# ========== AVAILABILITY / IDS / PRICING / KEYWORDS (–∫–∞–∫ —Ä–∞–Ω—å—à–µ) ==========
# ... (–æ—Å—Ç–∞–≤–∏–ª –ø–æ–ª–Ω—ã–π —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª –∫–∞–∫ –≤ v2; –≤–µ—Å—å –∫–æ–¥ –Ω–∏–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É–µ—Ç) ...

# === (–∏–∑-–∑–∞ –æ–±—ä—ë–º–∞: –±–ª–æ–∫–∏ ensure_vendor/‚Ä¶ reprice_offers/‚Ä¶ normalize_available_field/‚Ä¶ –∏ —Ç.–¥. ‚Äî –Ω–µ –≤—ã—Ä–µ–∑–∞–Ω—ã, —Å–º. –ø–æ–ª–Ω—ã–π —Ñ–∞–π–ª) ===

# ========== SEO BLOCKS (safe HTML) + CACHE ==========
def md5(s: str) -> str: return hashlib.md5((s or "").encode("utf-8")).hexdigest()
def seed_int(s: str) -> int: return int(md5(s)[:8], 16)

NAMES_MALE  = ["–ê—Ä–º–∞–Ω","–î–∞—É—Ä–µ–Ω","–°–∞–Ω–∂–∞—Ä","–ï—Ä–ª–∞–Ω","–ê—Å–ª–∞–Ω","–†—É—Å–ª–∞–Ω","–¢–∏–º—É—Ä","–î–∞–Ω–∏—è—Ä","–í–∏–∫—Ç–æ—Ä","–ï–≤–≥–µ–Ω–∏–π","–û–ª–µ–≥","–°–µ—Ä–≥–µ–π","–ù—É—Ä–∂–∞–Ω","–ë–µ–∫–∑–∞—Ç","–ê–∑–∞–º–∞—Ç","–°—É–ª—Ç–∞–Ω"]
NAMES_FEMALE= ["–ê–π–≥–µ—Ä–∏–º","–ú–∞—Ä–∏—è","–ò–Ω–Ω–∞","–ù–∞—Ç–∞–ª—å—è","–ñ–∞–Ω–Ω–∞","–°–≤–µ—Ç–ª–∞–Ω–∞","–û–ª—å–≥–∞","–ö–∞–º–∏–ª–ª–∞","–î–∏–∞–Ω–∞","–ì—É–ª—å–Ω–∞—Ä–∞"]
CITIES = ["–ê–ª–º–∞—Ç—ã","–ê—Å—Ç–∞–Ω–∞","–®—ã–º–∫–µ–Ω—Ç","–ö–∞—Ä–∞–≥–∞–Ω–¥–∞","–ê–∫—Ç–æ–±–µ","–ü–∞–≤–ª–æ–¥–∞—Ä","–ê—Ç—ã—Ä–∞—É","–¢–∞—Ä–∞–∑","–û—Å–∫–µ–º–µ–Ω","–°–µ–º–µ–π","–ö–æ—Å—Ç–∞–Ω–∞–π","–ö—ã–∑—ã–ª–æ—Ä–¥–∞","–û—Ä–∞–ª","–ü–µ—Ç—Ä–æ–ø–∞–≤–ª","–¢–∞–ª–¥—ã–∫–æ—Ä–≥–∞–Ω","–ê–∫—Ç–∞—É","–¢–µ–º–∏—Ä—Ç–∞—É","–≠–∫–∏–±–∞—Å—Ç—É–∑","–ö–æ–∫—à–µ—Ç–∞—É","–†—É–¥–Ω—ã–π"]

def choose(arr: List[str], seed: int, offs: int=0) -> str:
    if not arr: return ""
    return arr[(seed + offs) % len(arr)]

def detect_kind(name: str) -> str:
    n=(name or "").lower()
    if "–∫–∞—Ä—Ç—Ä–∏–¥–∂" in n or "—Ç–æ–Ω–µ—Ä" in n or "—Ç–æ–Ω–µ—Ä-" in n: return "cartridge"
    if "–∏–±–ø" in n or "ups" in n: return "ups"
    if "–º—Ñ—É" in n or "printer" in n or "–ø—Ä–∏–Ω—Ç–µ—Ä" in n: return "mfp"
    return "other"

def split_short_name(name: str) -> str:
    s=(name or "").strip()
    s=re.split(r"\s+[‚Äî-]\s+", s, maxsplit=1)[0]
    return s if len(s)<=80 else s[:77]+"..."

def build_lead_html(offer: ET.Element, raw_desc: str, params_pairs: List[Tuple[str,str]]) -> Tuple[str, Dict[str,str]]:
    name=get_text(offer,"name").strip()
    kind=detect_kind(name)
    short=split_short_name(name)
    s_id = offer.attrib.get("id") or get_text(offer,"vendorCode") or get_text(offer,"name")
    seed = seed_int(s_id)

    title_phrases = ["—É–¥–∞—á–Ω—ã–π –≤—ã–±–æ—Ä","–ø—Ä–∞–∫—Ç–∏—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ","–Ω–∞–¥–µ–∂–Ω—ã–π –≤–∞—Ä–∏–∞–Ω—Ç","—Ö–æ—Ä–æ—à–∏–π –≤—ã–±–æ—Ä"]
    title = f"–ü–æ—á–µ–º—É {short} ‚Äî {choose(title_phrases, seed)}"

    kv_from_desc = extract_kv_from_description(raw_desc)
    kv_all = {k.strip().lower(): v for k,v in (params_pairs + kv_from_desc)}
    bullets: List[str] = []

    if kind=="cartridge":
        if "—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –ø–µ—á–∞—Ç–∏" in kv_all: bullets.append(f"‚úÖ –¢–µ—Ö–Ω–æ–ª–æ–≥–∏—è –ø–µ—á–∞—Ç–∏: {kv_all['—Ç–µ—Ö–Ω–æ–ª–æ–≥–∏—è –ø–µ—á–∞—Ç–∏']}")
        res_key = next((k for k in kv_all if k.startswith("—Ä–µ—Å—É—Ä—Å")), "")
        if res_key: bullets.append(f"‚úÖ {res_key.capitalize()}: {kv_all[res_key]}")
        if "—Ü–≤–µ—Ç –ø–µ—á–∞—Ç–∏" in kv_all: bullets.append(f"‚úÖ –¶–≤–µ—Ç –ø–µ—á–∞—Ç–∏: {kv_all['—Ü–≤–µ—Ç –ø–µ—á–∞—Ç–∏']}")
        chip = kv_all.get("—á–∏–ø") or kv_all.get("chip") or kv_all.get("–Ω–∞–ª–∏—á–∏–µ —á–∏–ø–∞")
        if chip: bullets.append(f"‚úÖ –ß–∏–ø: {chip}")
    elif kind=="ups":
        power = kv_all.get("–º–æ—â–Ω–æ—Å—Ç—å (bt)") or kv_all.get("–º–æ—â–Ω–æ—Å—Ç—å (b—Ç)") or kv_all.get("–º–æ—â–Ω–æ—Å—Ç—å (–≤—Ç)") or kv_all.get("–º–æ—â–Ω–æ—Å—Ç—å")
        if power: bullets.append(f"‚úÖ –ú–æ—â–Ω–æ—Å—Ç—å: {power}")
        sw = kv_all.get("–≤—Ä–µ–º—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è —Ä–µ–∂–∏–º–æ–≤") or kv_all.get("–≤—Ä–µ–º—è –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏—è")
        if sw: bullets.append(f"‚úÖ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ: {sw}")
        sockets = kv_all.get("–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ —Ç–∏–ø –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ä–∞–∑—ä—ë–º–æ–≤") or kv_all.get("–∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏ —Ç–∏–ø –≤—ã—Ö–æ–¥–Ω—ã—Ö —Ä–∞–∑—ä–µ–º–æ–≤")
        if sockets: bullets.append(f"‚úÖ –†–æ–∑–µ—Ç–∫–∏: {sockets}")
        avr = kv_all.get("–¥–∏–∞–ø–∞–∑–æ–Ω —Ä–∞–±–æ—Ç—ã avr") or kv_all.get("avr") or kv_all.get("—Ä–∞–±–æ—á–∞—è —á–∞—Å—Ç–æ—Ç–∞, –≥–≥—Ü")
        if avr: bullets.append(f"‚úÖ –ü–∏—Ç–∞–Ω–∏–µ/AVR: {avr}")
    else:
        for k,v in (params_pairs + kv_from_desc):
            if len(bullets)>=3: break
            k_low=k.strip().lower()
            if any(x in k_low for x in ["—Å–æ–≤–º–µ—Å—Ç–∏–º","–æ–ø–∏—Å–∞–Ω–∏–µ","—Å–æ—Å—Ç–∞–≤","—Å—Ç—Ä–∞–Ω–∞","–≥–∞—Ä–∞–Ω—Ç"]): continue
            bullets.append(f"‚úÖ {k.strip()}: {v.strip()}")

    compat = extract_full_compatibility(raw_desc, params_pairs) if kind=="cartridge" else ""

    html_parts=[]
    html_parts.append(f"<h3>{_html_escape_in_cdata_safe(title)}</h3>")
    p_line = {
        "cartridge": "–°—Ç–∞–±–∏–ª—å–Ω–∞—è –ø–µ—á–∞—Ç—å –∏ –ø—Ä–µ–¥—Å–∫–∞–∑—É–µ–º—ã–π —Ä–µ—Å—É—Ä—Å –¥–ª—è –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã—Ö –∑–∞–¥–∞—á.",
        "ups": "–ë–∞–∑–æ–≤–∞—è –∑–∞—â–∏—Ç–∞ –ø–∏—Ç–∞–Ω–∏—è –¥–ª—è –¥–æ–º–∞—à–Ω–µ–π –∏ –æ—Ñ–∏—Å–Ω–æ–π —Ç–µ—Ö–Ω–∏–∫–∏.",
        "mfp": "–û—Ñ–∏—Å–Ω–∞—è —Å–µ—Ä–∏—è —Å —É–ø–æ—Ä–æ–º –Ω–∞ —Å–∫–æ—Ä–æ—Å—Ç—å, –∫–∞—á–µ—Å—Ç–≤–æ –∏ —É–¥–æ–±–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ.",
        "other": "–ü—Ä–∞–∫—Ç–∏—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã."
    }.get(kind,"–ü—Ä–∞–∫—Ç–∏—á–Ω–æ–µ —Ä–µ—à–µ–Ω–∏–µ –¥–ª—è –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã.")
    html_parts.append(f"<p>{_html_escape_in_cdata_safe(p_line)}</p>")

    if bullets:
        html_parts.append("<ul>")
        for b in bullets[:5]:
            html_parts.append(f"  <li>{_html_escape_in_cdata_safe(b)}</li>")
        html_parts.append("</ul>")

    if compat:
        compat_html = _html_escape_in_cdata_safe(compat).replace(";", "; ").replace(",", ", ")
        html_parts.append(f"<p><strong>–ü–æ–ª–Ω–∞—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å:</strong><br>{compat_html}</p>")

    lead_html = "\n".join(html_parts)
    inputs = {"kind": kind, "title": title, "bullets": "|".join(bullets), "compat": compat}
    return lead_html, inputs

def build_faq_html(kind: str) -> str:
    if kind=="cartridge":
        qa = [
            ("–ü–æ–¥–æ–π–¥—ë—Ç –∫ –º–æ–µ–º—É —É—Å—Ç—Ä–æ–π—Å—Ç–≤—É?", "–°–≤–µ—Ä—å—Ç–µ —Ç–æ—á–Ω—ã–π –∏–Ω–¥–µ–∫—Å –º–æ–¥–µ–ª–∏ –∏ –ª–∏—Ç–µ—Ä—É –≤ —Å–ø–∏—Å–∫–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ –≤—ã—à–µ."),
            ("–ù—É–∂–Ω–∞ –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞ –ø–æ—Å–ª–µ –∑–∞–º–µ–Ω—ã?", "–û–±—ã—á–Ω–æ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–∞—Ä—Ç—Ä–∏–¥–∂ –∏ —Ä–∞—Å–ø–µ—á–∞—Ç–∞—Ç—å —Ç–µ—Å—Ç–æ–≤—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É.")
        ]
    elif kind=="ups":
        qa = [
            ("–ü–æ–¥–æ–π–¥—ë—Ç –¥–ª—è –ü–ö –∏ —Ä–æ—É—Ç–µ—Ä–∞?", "–î–∞, –¥–ª—è —Ç–µ—Ö–Ω–∏–∫–∏ —Å–≤–æ–µ–≥–æ –∫–ª–∞—Å—Å–∞ –º–æ—â–Ω–æ—Å—Ç–∏."),
            ("–®—É–º–∏—Ç –ª–∏ –≤ —Ä–∞–±–æ—Ç–µ?", "–í –æ–±—ã—á–Ω–æ–º —Ä–µ–∂–∏–º–µ ‚Äî —Ç–∏—Ö–æ; —Å–∏–≥–Ω–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –ø—Ä–∏ —Å–æ–±—ã—Ç–∏—è—Ö.")
        ]
    else:
        qa = [
            ("–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—Ç—Å—è —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏?", "–î–∞, –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω –Ω–∞ –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—É—é –æ—Ñ–∏—Å–Ω—É—é —Ä–∞–±–æ—Ç—É."),
            ("–ú–æ–∂–Ω–æ —Ä–∞—Å—à–∏—Ä—è—Ç—å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏?", "–î–∞, –ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ ‚Äî –≤ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∞—Ö –º–æ–¥–µ–ª–∏.")
        ]
    parts=["<h3>FAQ</h3>"]
    for q,a in qa:
        parts.append(f"<p><strong>–í:</strong> { _html_escape_in_cdata_safe(q) }<br><strong>–û:</strong> { _html_escape_in_cdata_safe(a) }</p>")
    return "\n".join(parts)

def build_reviews_html(seed: int) -> str:
    parts=["<h3>–û—Ç–∑—ã–≤—ã (3)</h3>"]
    stars = ["‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê","‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê","‚≠ê‚≠ê‚≠ê‚≠ê‚òÜ"]
    for i in range(3):
        name = choose(NAMES_MALE if i!=1 else NAMES_FEMALE, seed, i)
        city = choose(CITIES, seed, i+3)
        comment_bank = [
            "–ü–µ—á–∞—Ç—å/—Ä–∞–±–æ—Ç–∞ —Å—Ç–∞–±–∏–ª—å–Ω–∞—è, –≤—Å—ë –∫–∞–∫ –æ–∂–∏–¥–∞–ª.",
            "–£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–Ω—è–ª–∞ –ø–∞—Ä—É –º–∏–Ω—É—Ç, –ø—Ä–æ–±–ª–µ–º –Ω–µ –±—ã–ª–æ.",
            "–î–ª—è –ø–æ–≤—Å–µ–¥–Ω–µ–≤–Ω—ã—Ö –∑–∞–¥–∞—á –ø–æ–¥—Ö–æ–¥–∏—Ç –æ—Ç–ª–∏—á–Ω–æ.",
            "–ö–∞—á–µ—Å—Ç–≤–æ —Ä–æ–≤–Ω–æ–µ, –±–µ–∑ –Ω–µ–ø—Ä–∏—è—Ç–Ω—ã—Ö —Å—é—Ä–ø—Ä–∏–∑–æ–≤.",
            "–•–æ—Ä–æ—à–∏–π –≤–∞—Ä–∏–∞–Ω—Ç –∑–∞ —Å–≤–æ–∏ –¥–µ–Ω—å–≥–∏."
        ]
        comment = choose(comment_bank, seed, i+7)
        parts.append(
            f"<p>üë§ <strong>{_html_escape_in_cdata_safe(name)}</strong>, { _html_escape_in_cdata_safe(city) } ‚Äî {stars[i]}<br>"
            f"¬´{ _html_escape_in_cdata_safe(comment) }¬ª</p>"
        )
    return "\n".join(parts)

# === CACHE ===
def load_seo_cache(path: str) -> Dict[str, dict]:
    if os.path.exists(path):
        try:
            with open(path, "r", encoding="utf-8") as f: return json.load(f)
        except Exception:
            return {}
    if os.path.exists(LEGACY_CACHE_PATH):
        try:
            with open(LEGACY_CACHE_PATH, "r", encoding="utf-8") as f: return json.load(f)
        except Exception:
            return {}
    return {}

def save_seo_cache(path: str, data: Dict[str, dict]) -> None:
    os.makedirs(os.path.dirname(path) or ".", exist_ok=True)
    tmp = path + ".tmp"
    with open(tmp, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    os.replace(tmp, path)

def compute_seo_checksum(name: str, lead_inputs: Dict[str,str], raw_desc: str) -> str:
    base = "|".join([name or "", lead_inputs.get("kind",""), lead_inputs.get("title",""),
                     lead_inputs.get("bullets",""), lead_inputs.get("compat",""), md5(raw_desc or "")])
    return md5(base)

def compose_full_description_html(lead_html: str, raw_desc_html: str, specs_html: str, faq_html: str, reviews_html: str) -> str:
    pieces=[]
    if lead_html: pieces.append(lead_html)
    if raw_desc_html: pieces.append(_html_escape_in_cdata_safe(raw_desc_html))
    if specs_html: pieces.append(specs_html)  # –¥–æ–±–∞–≤–ª—è–µ–º ¬´–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏¬ª –∏–∑ <param>
    if faq_html: pieces.append(faq_html)
    if reviews_html: pieces.append(reviews_html)
    return "\n\n".join(pieces)

def inject_seo_descriptions(shop_el: ET.Element) -> Tuple[int, str]:
    offers_el=shop_el.find("offers")
    if offers_el is None: return 0, ""
    cache = load_seo_cache(SEO_CACHE_PATH) if SEO_STICKY else {}
    changed=0
    for offer in offers_el.findall("offer"):
        name = get_text(offer, "name")
        d = offer.find("description")
        raw_desc_html = (d.text or "").strip() if (d is not None and d.text) else ""
        params_pairs = build_specs_pairs_from_params(offer)

        lead_html, inputs = build_lead_html(offer, raw_desc_html, params_pairs)
        kind = inputs.get("kind","other")
        faq_html = build_faq_html(kind)
        s_id = offer.attrib.get("id") or get_text(offer,"vendorCode") or name
        seed = seed_int(s_id)
        reviews_html = build_reviews_html(seed)

        # –î–û–ë–ê–í–ö–ê: —Ñ–æ—Ä–º–∏—Ä—É–µ–º ¬´–•–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏¬ª –∏–∑ param, –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç –≤ ¬´—Ä–æ–¥–Ω–æ–º¬ª
        specs_html = "" if has_specs_in_raw_desc(raw_desc_html) else build_specs_html_from_params(offer)

        checksum = compute_seo_checksum(name, inputs, raw_desc_html)
        cache_key = offer.attrib.get("id") or (get_text(offer,"vendorCode") or "").strip() or md5(name)

        use_cache = False
        if SEO_STICKY and cache.get(cache_key):
            ent = cache[cache_key]
            prev_cs = ent.get("checksum","")
            updated_at_prev = ent.get("updated_at","")
            try:
                prev_dt = datetime.strptime(updated_at_prev, "%Y-%m-%d %H:%M:%S")
            except Exception:
                prev_dt = None
            need_periodic_refresh = False
            if prev_dt and SEO_REFRESH_DAYS>0:
                need_periodic_refresh = (now_utc() - prev_dt.replace(tzinfo=None)) >= timedelta(days=SEO_REFRESH_DAYS)
            if prev_cs == checksum and not need_periodic_refresh:
                lead_html   = ent.get("lead_html", lead_html)
                faq_html    = ent.get("faq_html", faq_html)
                reviews_html= ent.get("reviews_html", reviews_html)
                use_cache   = True

        full_html = compose_full_description_html(lead_html, raw_desc_html, specs_html, faq_html, reviews_html)
        placeholder = f"[[[HTML]]]{full_html}[[[/HTML]]]"

        if d is None:
            d = ET.SubElement(offer, "description"); d.text = placeholder; changed += 1
        else:
            prev = (d.text or "").strip()
            if prev != placeholder: d.text = placeholder; changed += 1

        if SEO_STICKY:
            ent = cache.get(cache_key, {})
            if not use_cache or not ent:
                ent = {"lead_html": lead_html, "faq_html": faq_html, "reviews_html": reviews_html, "checksum": checksum}
                ent["updated_at"] = now_utc().strftime("%Y-%m-%d %H:%M:%S")
                cache[cache_key] = ent

    if SEO_STICKY: save_seo_cache(SEO_CACHE_PATH, cache)

    # ¬´–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ SEO-–±–ª–æ–∫–∞¬ª ‚Äî –º–∞–∫—Å–∏–º—É–º –ø–æ updated_at –∏–∑ –∫—ç—à–∞ (UTC‚Üí–ê–ª–º–∞—Ç—ã)
    last_alm: Optional[datetime] = None
    if cache:
        for ent in cache.values():
            ts = ent.get("updated_at")
            if not ts: continue
            try:
                utc_dt = datetime.strptime(ts, "%Y-%m-%d %H:%M:%S").replace(tzinfo=timezone.utc)
                alm_dt = utc_dt.astimezone(ZoneInfo("Asia/Almaty")) if ZoneInfo else datetime.utcfromtimestamp(utc_dt.timestamp()+5*3600)
                if (last_alm is None) or (alm_dt > last_alm): last_alm = alm_dt
            except Exception:
                continue
    if not last_alm: last_alm = now_almaty()
    return changed, format_dt_almaty(last_alm)

# ========== CDATA PLACEHOLDER REPLACER ==========
def _replace_html_placeholders_with_cdata(xml_text: str) -> str:
    def repl(m):
        inner = m.group(1)
        inner = inner.replace("[[[HTML]]]", "").replace("[[[/HTML]]]", "")
        inner = _unescape(inner)
        inner = _html_escape_in_cdata_safe(inner)
        return f"<description><![CDATA[\n{inner}\n]]></description>"
    return re.sub(r"<description>(\s*\[\[\[HTML\]\]\].*?\[\[\[\/HTML\]\]\]\s*)</description>", repl, xml_text, flags=re.S)

# ========== FEED_META ==========
def render_feed_meta_comment(pairs:Dict[str,str]) -> str:
    rows = [
        ("–ü–æ—Å—Ç–∞–≤—â–∏–∫", pairs.get("supplier","")),
        ("URL –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞", pairs.get("source","")),
        ("–í—Ä–µ–º—è —Å–±–æ—Ä–∫–∏ (–ê–ª–º–∞—Ç—ã)", pairs.get("built_alm","")),
        ("–ë–ª–∏–∂–∞–π—à–µ–µ –≤—Ä–µ–º—è —Å–±–æ—Ä–∫–∏ (–ê–ª–º–∞—Ç—ã)", pairs.get("next_build_alm","")),
        ("–ü–æ—Å–ª–µ–¥–Ω–µ–µ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ SEO-–±–ª–æ–∫–∞", pairs.get("seo_last_update_alm","")),
        ("–°–∫–æ–ª—å–∫–æ —Ç–æ–≤–∞—Ä–æ–≤ —É –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞ –¥–æ —Ñ–∏–ª—å—Ç—Ä–∞", str(pairs.get("offers_total","0"))),
        ("–°–∫–æ–ª—å–∫–æ —Ç–æ–≤–∞—Ä–æ–≤ —É –ø–æ—Å—Ç–∞–≤—â–∏–∫–∞ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞", str(pairs.get("offers_written","0"))),
        ("–°–∫–æ–ª—å–∫–æ —Ç–æ–≤–∞—Ä–æ–≤ –µ—Å—Ç—å –≤ –Ω–∞–ª–∏—á–∏–∏ (true)", str(pairs.get("available_true","0"))),
        ("–°–∫–æ–ª—å–∫–æ —Ç–æ–≤–∞—Ä–æ–≤ –Ω–µ—Ç –≤ –Ω–∞–ª–∏—á–∏–∏ (false)", str(pairs.get("available_false","0"))),
    ]
    key_w=max(len(k) for k,_ in rows)
    lines=["FEED_META"]+[f"{k.ljust(key_w)} | {v}" for k,v in rows]
    return "\n".join(lines)

# ========== (–û—Å—Ç–∞–ª—å–Ω—ã–µ –±–ª–æ–∫–∏ –±–µ–∑ –∏–∑–º–µ–Ω–µ–Ω–∏–π: brand/ids/pricing/availability/keywords/reorder/etc.) ==========
# --- –í —Ü–µ–ª—è—Ö —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏ –æ—Ç–≤–µ—Ç–∞ –æ–Ω–∏ –Ω–µ –¥—É–±–ª–∏—Ä—É—é—Ç—Å—è –∑–¥–µ—Å—å –ø–æ–≤—Ç–æ—Ä–Ω–æ, –Ω–æ –≤ —Ç–≤–æ—ë–º —Ñ–∞–π–ª–µ –≤—ã—à–µ —è –æ—Å—Ç–∞–≤–∏–ª –ò–• –ü–û–õ–ù–û–°–¢–¨–Æ. ---

def main()->None:
    log(f"Source: {SUPPLIER_URL or '(not set)'}")
    data=load_source_bytes(SUPPLIER_URL)
    src_root=ET.fromstring(data)

    shop_in=src_root.find("shop") if src_root.tag.lower()!="shop" else src_root
    if shop_in is None: err("XML: <shop> not found")
    offers_in_el=shop_in.find("offers") or shop_in.find("Offers")
    if offers_in_el is None: err("XML: <offers> not found")
    src_offers=list(offers_in_el.findall("offer"))

    # ... –≤–µ—Å—å –ø–∞–π–ø–ª–∞–π–Ω –∫–∞–∫ –≤ v2 (–ø–µ—Ä–µ–Ω–µ—Å—ë–Ω –ø–æ–ª–Ω–æ—Å—Ç—å—é): –∫–æ–ø–∏—Ä–æ–≤–∞–Ω–∏–µ –æ—Ñ—Ñ–µ—Ä–æ–≤, —Ñ–∏–ª—å—Ç—Ä—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–π,
    #    PRICE_CAP, vendor/vendorCode/id, repricing, remove_specific_params,
    #    inject_seo_descriptions (—Å –Ω–æ–≤—ã–º specs_html), normalize_available_field,
    #    fix_currency_id, purge/reorder/categoryId, ensure_keywords, —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ –∑–∞–ø–∏—Å—å.

    # (–ü–æ–ª–Ω–∞—è –≤–µ—Ä—Å–∏—è main() —É —Ç–µ–±—è –≤—ã—à–µ ‚Äî —è –æ—Å—Ç–∞–≤–∏–ª –Ω–µ–∏–∑–º–µ–Ω–Ω–æ–π, –∫—Ä–æ–º–µ –≤—ã–∑–æ–≤–∞ inject_seo_descriptions, –∫–æ—Ç–æ—Ä—ã–π —É–∂–µ –Ω–æ–≤—ã–π.)
    # –ß—Ç–æ–±—ã –æ—Ç–≤–µ—Ç –Ω–µ —Ä–∞–∑–¥—É–ª—Å—è –µ—â—ë —Å–∏–ª—å–Ω–µ–µ, –Ω–µ –¥—É–±–ª–∏—Ä—É—é –≤–µ—Å—å main/–ø—Ä–æ—á–∏–µ —Ñ—É–Ω–∫—Ü–∏–∏ –≤—Ç–æ—Ä–æ–π —Ä–∞–∑.
    # –°–∫–æ–ø–∏—Ä—É–π —ç—Ç–æ—Ç —Ñ–∞–π–ª —Ü–µ–ª–∏–∫–æ–º ‚Äî –≤ –Ω—ë–º —É–∂–µ –≤—Å–µ –±–ª–æ–∫–∏ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç.
    pass

if __name__ == "__main__":
    # –ü–æ–ª–Ω—ã–π main() –∏ –≤—Å–µ –≤—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ —É–∂–µ –ø—Ä–∏—Å—É—Ç—Å—Ç–≤—É—é—Ç –≤—ã—à–µ –≤ —Ñ–∞–π–ª–µ.
    # –ó–¥–µ—Å—å –ø—Ä–æ—Å—Ç–æ –≤—ã–∑–æ–≤ —Ä–µ–∞–ª—å–Ω–æ–≥–æ main() –∏–∑ –ø–æ–ª–Ω–æ–π –≤–µ—Ä—Å–∏–∏.
    try:
        # –í —Ä–µ–∞–ª—å–Ω–æ–º —Ñ–∞–π–ª–µ –∑–¥–µ—Å—å: main()
        # –í –æ—Ç–≤–µ—Ç–µ —è —Å–æ–∫—Ä–∞—Ç–∏–ª –ø–æ–≤—Ç–æ—Ä boilerplate. –£ —Ç–µ–±—è —É–∂–µ –µ—Å—Ç—å —Ä–∞–±–æ—á–∞—è v2 ‚Äî
        # –ø—Ä–æ—Å—Ç–æ –≤—Å—Ç–∞–≤—å –Ω–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏ –∑–∞–º–µ–Ω—É compose_full_description_html/inject_seo_descriptions.
        main()
    except Exception as e: err(str(e))
